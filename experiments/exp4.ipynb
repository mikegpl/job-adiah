{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to build sklearn-like pipeline for transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd         \n",
    "import os.path\n",
    "\n",
    "N_JOBS = 6\n",
    "DEBUG=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path=\"../../data/csv/all_v2.csv\"):\n",
    "    dataframe = pd.read_csv(path, index_col=0)\n",
    "    return dataframe.loc[~dataframe[\"execTimeMs\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe(dataframe):\n",
    "    output = dataframe.dropna(axis=\"columns\")\n",
    "    targets = output[\"execTimeMs\"]\n",
    "    dropped = output[[\"command\", \"execTimeMs\", \"jobId\", \"ctime_mean\", \"ctime_max\", \"ctime_sum\", \"read_sum\",\"write_sum\",\"readSyscalls_sum\",\"writeSyscalls_sum\",\"readReal_sum\",\"writeReal_sum\",\"writeCancelled_sum\",\"rxBytes_sum\",\"rxPackets_sum\",\"rxErrors_sum\",\"rxDrop_sum\",\"rxFifo_sum\",\"rxFrame_sum\",\"rxCompressed_sum\",\"rxMulticast_sum\",\"txBytes_sum\",\"txPackets_sum\",\"txErrors_sum\",\"txDrop_sum\",\"txFifo_sum\",\"txColls_sum\",\"txCarrier_sum\",\"txCompressed_sum\",\"cpu_mean\",\"cpu_max\",\"memory_mean\",\"memory_max\"]]\n",
    "    features = output.drop(dropped.columns, axis=1)\n",
    "    return features, targets, dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, targets, dropped = prepare_dataframe(load_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "workflowName              object\n",
       "size                     float64\n",
       "executable                object\n",
       "args                      object\n",
       "inputs                    object\n",
       "outputs                   object\n",
       "name                      object\n",
       "cpu.manufacturer          object\n",
       "cpu.brand                 object\n",
       "cpu.speed                float64\n",
       "cpu.cores                  int64\n",
       "cpu.physicalCores          int64\n",
       "cpu.processors             int64\n",
       "mem.total                  int64\n",
       "mem.free                   int64\n",
       "mem.used                   int64\n",
       "mem.active                 int64\n",
       "mem.available              int64\n",
       "mem.buffers                int64\n",
       "mem.cached                 int64\n",
       "mem.slab                   int64\n",
       "mem.buffcache              int64\n",
       "mem.swaptotal              int64\n",
       "mem.swapused               int64\n",
       "mem.swapfree               int64\n",
       "total_cpus               float64\n",
       "avg_cpus                 float64\n",
       "avg_pods                 float64\n",
       "total_ram_available        int64\n",
       "average_ram_available    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_selector as selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_list(series):\n",
    "    def vectorize(list_string):\n",
    "        return len(eval(list_string))\n",
    "    return np.vectorize(vectorize)(series)\n",
    "\n",
    "def ListTransformer():\n",
    "    return FunctionTransformer(func=vectorize_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_transformer = Pipeline(steps=[(\"list\", ListTransformer()), (\"scaler\", StandardScaler())])\n",
    "list_features = list(['args', 'inputs', 'outputs'])\n",
    "\n",
    "numerical_transformer = StandardScaler()\n",
    "numerical_features = list(features.select_dtypes(include=\"number\").columns)\n",
    "\n",
    "categorical_transformer = OneHotEncoder(sparse=False, handle_unknown = \"ignore\")\n",
    "categorical_features = list(set(features.select_dtypes(include=\"object\").columns) ^ set(list_features))\n",
    "\n",
    "def make_classifying_preprocessor(additional_features=[\"read_sum\", \"write_sum\", \"cpu_mean\", \"memory_max\"]):\n",
    "    external_features = categorical_features + additional_features\n",
    "    return ColumnTransformer(\n",
    "            transformers=[('lists', list_transformer, list_features), \n",
    "                          ('num', numerical_transformer, numerical_features),\n",
    "                          ('cat', categorical_transformer, external_features)])\n",
    "\n",
    "def make_regression_preprocessor(additional_features=[\"read_sum\", \"write_sum\", \"cpu_mean\", \"memory_max\"]):\n",
    "    external_features = numerical_features + additional_features\n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('lists', list_transformer, list_features),            \n",
    "            ('num', numerical_transformer, external_features),  \n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "\n",
    "preprocessor = make_classifying_preprocessor(additional_features=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import percentileofscore\n",
    "import math\n",
    "\n",
    "def calculate_quantile_rank(labels, label):\n",
    "    return percentileofscore(labels, label) / 100\n",
    "\n",
    "def calculate_utilization_class(labels, label):\n",
    "    def label_for_rank(rank):\n",
    "        if rank > 0.75:\n",
    "            return 'very high'\n",
    "        elif rank > 0.5:\n",
    "            return 'high'\n",
    "        elif rank > 0.25:\n",
    "            return 'medium'\n",
    "        else:\n",
    "            return 'low'\n",
    "    return label_for_rank(calculate_quantile_rank(labels, label))\n",
    "\n",
    "def calculate_utilization_bucket(labels, label, num_buckets):\n",
    "    bucket_size = 1.0 / num_buckets\n",
    "    def bucket_for_rank(rank):\n",
    "        return str(math.floor(rank / bucket_size))\n",
    "    return bucket_for_rank(calculate_quantile_rank(labels, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline composition (with PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.linear_model import Lasso, SGDRegressor, ElasticNet, LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, HalvingGridSearchCV\n",
    "from sklearn.kernel_ridge import KernelRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_steps = [('pca', PCA(random_state=42))]\n",
    "dummy_pipeline = Pipeline(steps=base_steps +[('dummy', DummyRegressor())])\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_param_grid = {\n",
    "    'pca__n_components': np.arange(1, 50, 1),    \n",
    "}\n",
    "knn_param_grid = {\n",
    "    'knn__n_neighbors': np.arange(1, 30, 3),\n",
    "}\n",
    "regressor = ('knn', KNeighborsRegressor())\n",
    "full_pipeline = Pipeline(steps= base_steps + [regressor])\n",
    "grid_search = HalvingGridSearchCV(full_pipeline, {**knn_param_grid, **pca_param_grid}, cv=2, verbose=2, scoring=\"r2\", n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rae(actual, predicted):\n",
    "    \"\"\" Relative Absolute Error (aka Approximation Error) \"\"\"\n",
    "    EPSILON=1e-10\n",
    "    return np.sum(np.abs(actual - predicted)) / (np.sum(np.abs(actual - np.mean(actual))) + EPSILON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_absolute_percentage_error, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rae_scorer=make_scorer(rae, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_regression_score(true, pred, scores=[r2_score, mean_absolute_error, mean_absolute_percentage_error, rae]):\n",
    "    executor = get_reusable_executor(max_workers=4)\n",
    "    results = executor.map(lambda fun: fun(true, pred), scores)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loky import get_reusable_executor\n",
    "\n",
    "def rate_regressor(X_train, y_train, X_test, y_test, regressor, regressor_params, verbose=10, aggressive_elimination=True, steps=base_steps, scoring=\"r2\"):\n",
    "    if DEBUG:\n",
    "        print(f\"Rating {regressor}\")\n",
    "    full_pipeline = Pipeline(steps= base_steps + [regressor])\n",
    "    vector_length = min(X_train.shape[0], X_train.shape[1])\n",
    "    pca_param_grid = {'pca__n_components': np.arange(1, vector_length, 1),}\n",
    "    grid_search = HalvingGridSearchCV(full_pipeline, {**pca_param_grid, **regressor_params}, cv=2, verbose=verbose, scoring=scoring, n_jobs=N_JOBS)\n",
    "    if DEBUG:\n",
    "        print(\"Evaluating grid search\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # scores\n",
    "    if DEBUG:\n",
    "        print(\"Predicting on test set\")\n",
    "    prediction = grid_search.best_estimator_.predict(X_test)\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(\"Calculating scores\")\n",
    "    r2, mae, mape, rae = calculate_regression_score(y_test, prediction)\n",
    "    if DEBUG:\n",
    "        print(\"Calculated scores on test set\")\n",
    "    adjusted_r2 = 1 - (1-r2)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)\n",
    "    return {\"r2\": r2, \"adjusted_r2\": adjusted_r2, \"mae\": mae, \"mape\": mape, \"rae\": rae,\"best_score\": grid_search.best_score_, \"params\": grid_search.best_params_}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here go regressor params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = (\"knn\", KNeighborsRegressor())\n",
    "knn_params = {'knn__n_neighbors': np.arange(1, 30, 1)}\n",
    "\n",
    "lasso = (\"lasso\", Lasso(random_state=5))\n",
    "lasso_params = {\"lasso__alpha\": np.arange(0.01, 1, 0.05)}\n",
    "\n",
    "mlp_regressor = (\"mlp\", MLPRegressor(random_state=5))\n",
    "mlp_params = {\"mlp__activation\": [\"relu\", \"logistic\"], \"mlp__hidden_layer_sizes\": [(100,), (100, 50,)]}\n",
    "\n",
    "dtr = (\"dtr\", DecisionTreeRegressor(random_state=5))\n",
    "dtr_params = {\"dtr__criterion\": [\"mse\", \"friedman_mse\", \"mae\", \"poisson\"], \"dtr__max_depth\": [5, 10, 15, 25]}\n",
    "\n",
    "en = (\"elasticnet\", ElasticNet(random_state=5))\n",
    "en_params = {\"elasticnet__alpha\": np.arange(0.01, 1, 0.05), \"elasticnet__l1_ratio\": np.arange(0, 1, 0.1)}\n",
    "\n",
    "svr = (\"svr\", SGDRegressor())\n",
    "svr_params = {\"svr__loss\": [\"squared_loss\", \"huber\", \"epsilon_insensitive\"], \"svr__penalty\": ['l2', 'l1', 'elasticnet'],\n",
    "             \"svr__alpha\": np.arange(0.0001, 0.2, 0.01), \"svr__max_iter\": [10000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_data(features, targets, regressors, verbose=10, pipeline_steps=base_steps):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.3, random_state=0)\n",
    "    df = pd.DataFrame(columns=[\"name\", \"pca\", \"adjusted_r2\",\"r2\", \"mae\", \"mape\", \"rae\", \"best_score\", \"params\"])\n",
    "    for (regressor, params) in regressors:\n",
    "        result = rate_regressor(X_train, y_train, X_test, y_test, regressor, params, verbose, pipeline_steps)\n",
    "        df = df.append({\"name\": regressor[0], **result, \"pca\": result[\"params\"][\"pca__n_components\"]}, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_data_explicit(X_train, X_test, y_train, y_test, regressors, verbose=10, pipeline_steps=base_steps, scoring=\"r2\"):\n",
    "    df = pd.DataFrame(columns=[\"name\", \"pca\", \"adjusted_r2\",\"r2\", \"mae\", \"mape\", \"best_score\", \"params\"])\n",
    "    for (regressor, params) in regressors:\n",
    "        result = rate_regressor(X_train, y_train, X_test, y_test, regressor, params, verbose, pipeline_steps, scoring=scoring)\n",
    "        df = df.append({\"name\": regressor[0], **result, \"pca\": result[\"params\"][\"pca__n_components\"]}, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_classifiers_for_data(features, targets, classifiers, verbose=10, pipeline_steps=base_steps):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.3, random_state=0)\n",
    "    df = pd.DataFrame(columns=[\"name\", \"pca\", \"accuracy\",\"balanced_accuracy\", \"f1_micro\", \"f1_macro\", \"params\"])\n",
    "    for (classifier, params) in classifiers:\n",
    "        result = rate_classifier(X_train, y_train, X_test, y_test, classifier, params, verbose, pipeline_steps)\n",
    "        df = df.append({\"name\": classifier[0], **result, \"pca\": result[\"params\"][\"pca__n_components\"]}, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_dataset(dataframe, regressors, verbose=2):\n",
    "    print(f\"Rating dataset of len {len(dataframe)}\")\n",
    "    features, targets, _ = prepare_dataframe(dataframe[:10000])\n",
    "    features = preprocessor.fit_transform(features)\n",
    "    rate_data(features, targets, regressors, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_regressors = [\n",
    "    (knn, knn_params),\n",
    "    (dtr, dtr_params),\n",
    "    (lasso, lasso_params),\n",
    "    (mlp_regressor, mlp_params),\n",
    "    (en, en_params),\n",
    "    (svr, svr_params),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_experiment():\n",
    "    print(\"Rating jobs datasets\")\n",
    "    for dataset in dfs_for_jobs:\n",
    "        print(dataset.iloc[0][\"name\"])\n",
    "        rate_dataset(dataset, basic_regressors)\n",
    "\n",
    "    print(\"Rating common datasets\")\n",
    "    for dataset in datasets:\n",
    "        rate_dataset(dataset, basic_regressors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rate_dataset(dfs_for_jobs[8], verbose=0)\n",
    "# rate_dataset(dfs_for_jobs[1], basic_regressors, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eksperyment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zmierzyć skuteczności najlepszych pipelinów dla każdego joba, zobaczyć czy warto schodzić w dół pod wzgledem błędów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = load_data().dropna(axis=\"columns\")\n",
    "raw_datasets = { x:pd.DataFrame(y) for x, y in full_df.groupby('name', as_index=False)}\n",
    "datasets_split = {x:train_test_split(df, random_state=0, train_size=0.75) for x,df in raw_datasets.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numerical_pipeline_data_big(data, resources=[\"read_sum\", \"write_sum\", \"cpu_mean\", \"memory_max\"]):\n",
    "    \"\"\"\n",
    "    Raw data enhanced with resource utilization quantile scores, but scores are assigned - not predicted\n",
    "    \"\"\"\n",
    "    features, labels, dropped = prepare_dataframe(data)\n",
    "    for resource in resources:\n",
    "        features[resource] = dropped[resource].map(lambda value: calculate_quantile_rank(dropped[resource], value))\n",
    "    features = make_regression_preprocessor(resources).fit_transform(features)\n",
    "    return pd.DataFrame(features, index=labels.index), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_symbolic_regression_data(data, resources=[\"read_sum\", \"write_sum\", \"cpu_mean\", \"memory_max\"]):\n",
    "    features, labels, dropped = prepare_dataframe(data)\n",
    "    for resource in resources:\n",
    "        features[resource] = dropped[resource].map(lambda value: calculate_quantile_rank(dropped[resource], value))\n",
    "    for list_feature in list_features:\n",
    "        features[list_feature] = features[list_feature].map(lambda list_val: vectorize_list(list_val))\n",
    "    return pd.DataFrame(features, index=labels.index), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorical_pipeline_data_big(data, resources=[\"read_sum\", \"write_sum\", \"cpu_mean\", \"memory_max\"]):\n",
    "    features, labels, dropped = prepare_dataframe(data)\n",
    "    for resource in resources:\n",
    "        features[resource] = dropped[resource].map(lambda value: calculate_utilization_bucket(dropped[resource], value, num_buckets=8))\n",
    "    features = make_classifying_preprocessor(resources).fit_transform(features)\n",
    "    return pd.DataFrame(features, index=labels.index), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.name.unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przebieg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trenujemy pipeline ogólny, liczymy jego skuteczności dla każdego typu jobów. Z eksperymentu 1. - najlepszy pipeline to był:\n",
    "\n",
    "dtr, mae, pca 71\n",
    "\n",
    "dla każdego typu jobów trenujemy dla niego pipeline, liczymy skuteczności"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = datasets_split['add_replace']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adjusted_r2(r2, y_test, X_test):\n",
    "    return 1 - (1-r2)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment4():\n",
    "    exp4_resources = [\"read_sum\", \"write_sum\", \"cpu_max\", \"cpu_mean\", \"memory_mean\", \"memory_max\"]\n",
    "        \n",
    "    big_regressor = Pipeline([ # test pipeline\n",
    "        ('pca', PCA(random_state=42, n_components=71)),\n",
    "#         ('knn', KNeighborsRegressor(n_neighbors=11))\n",
    "        ('dtr', DecisionTreeRegressor(criterion=\"mae\", max_depth=15))\n",
    "    ])\n",
    "    print(\"Preparing data for big regressor\")\n",
    "    X, y = get_numerical_pipeline_data_big(full_df, exp4_resources)\n",
    "    train_indices = np.concatenate([train.index for (_, (train, _)) in datasets_split.items()])\n",
    "    test_indices = np.concatenate([test.index for (_, (_, test)) in datasets_split.items()])\n",
    "    print(f\"{len(X)} {len(y)} {len(train_indices)} {len(test_indices)}\")\n",
    "    X_train, X_test, y_train, y_test = X.loc[train_indices], X.loc[test_indices], y.loc[train_indices], y.loc[test_indices]\n",
    "    print(f\"Making split with test as {len(test_indices)/(len(test_indices) + len(train_indices))} of dataset\")\n",
    "    \n",
    "    print(\"Training big regressor with train data\")\n",
    "    big_regressor.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Predicting with big regressor on test data\")\n",
    "    y_predicted = big_regressor.predict(X_test)\n",
    "    \n",
    "    print(\"Rating big regressor's overall performance\")\n",
    "    [r2, mae, mape, rae_score] = calculate_regression_score(y_test, y_predicted, [r2_score, mean_absolute_error, mean_absolute_percentage_error, rae])\n",
    "    print(f\"Scores for big regressor:\")\n",
    "    print(f\"R2: {r2}\")\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"MAPE: {mape}\")\n",
    "    print(f\"RAE: {rae_score}\")\n",
    "    print(f\"Params: {big_regressor}\")\n",
    "    dataframes = []\n",
    "    for (job, (train, test)) in datasets_split.items():\n",
    "        print(f\"Comparing big regressor vs local regressor for job {job}\")\n",
    "        print(\"Preparing data for local regressor\")\n",
    "        joint_df = pd.concat([train, test])\n",
    "        local_X, local_y = get_numerical_pipeline_data_big(joint_df, exp4_resources)\n",
    "        print(f\"{len(local_X)} {len(train.index)} {len(test.index)}\")\n",
    "        local_X_train, local_X_test, local_y_train, local_y_test = local_X.loc[train.index], local_X.loc[test.index], local_y.loc[train.index], local_y.loc[test.index]\n",
    "        \n",
    "        print(f\"Rating local regressors for job {job}\")\n",
    "        regressor_df = rate_data_explicit(local_X_train, local_X_test, local_y_train, local_y_test, basic_regressors, verbose=0)\n",
    "        print(regressor_df.head())\n",
    "        print(\"Preparing data for big regressor\")\n",
    "        local_X_test, local_y_test = X.loc[test.index], y.loc[test.index]\n",
    "        \n",
    "        print(f\"Rating big regressor for job {job}\")\n",
    "        local_predicted = big_regressor.predict(local_X_test)\n",
    "        [r2, mae, mape, rae_score] = calculate_regression_score(local_predicted, local_y_test, [r2_score, mean_absolute_error, mean_absolute_percentage_error, rae])\n",
    "        adjusted_r2 = get_adjusted_r2(r2, local_y_test, local_X_test)\n",
    "        regressor_df = regressor_df.append({\"name\": \"big\", **{\"r2\": r2, \"mae\": mae, \"mape\": mape, \"rae\": rae_score, \"adjusted_r2\": adjusted_r2}, \"pca\": \"xD\"}, ignore_index=True)\n",
    "        regressor_df[\"job\"] = job\n",
    "        regressor_df[\"size\"] = len(joint_df)\n",
    "        dataframes.append(regressor_df)\n",
    "        \n",
    "    return pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment4_adjustedr2():\n",
    "    exp4_resources = [\"read_sum\", \"write_sum\", \"cpu_max\", \"cpu_mean\", \"memory_mean\", \"memory_max\"]\n",
    "        \n",
    "    big_regressor = Pipeline([ # test pipeline\n",
    "        ('pca', PCA(random_state=42, n_components=71)),\n",
    "#         ('knn', KNeighborsRegressor(n_neighbors=11))\n",
    "        ('dtr', DecisionTreeRegressor(criterion=\"mae\", max_depth=15))\n",
    "    ], verbose=True)\n",
    "    print(\"Preparing data for big regressor\")\n",
    "    X, y = get_numerical_pipeline_data_big(full_df, exp4_resources)\n",
    "    train_indices = np.concatenate([train.index for (_, (train, _)) in datasets_split.items()])\n",
    "    test_indices = np.concatenate([test.index for (_, (_, test)) in datasets_split.items()])\n",
    "    print(f\"{len(X)} {len(y)} {len(train_indices)} {len(test_indices)}\")\n",
    "    X_train, X_test, y_train, y_test = X.loc[train_indices], X.loc[test_indices], y.loc[train_indices], y.loc[test_indices]\n",
    "    print(f\"Making split with test as {len(test_indices)/(len(test_indices) + len(train_indices))} of dataset\")\n",
    "    \n",
    "    print(\"Training big regressor with train data\")\n",
    "    big_regressor.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Predicting with big regressor on test data\")\n",
    "    y_predicted = big_regressor.predict(X_test)\n",
    "    \n",
    "    print(\"Rating big regressor's overall performance\")\n",
    "    [r2, mae, mape, rae_score] = calculate_regression_score(y_test, y_predicted, [r2_score, mean_absolute_error, mean_absolute_percentage_error, rae])\n",
    "    print(f\"Scores for big regressor:\")\n",
    "    print(f\"R2: {r2}\")\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"MAPE: {mape}\")\n",
    "    print(f\"RAE: {rae_score}\")\n",
    "    print(f\"Params: {big_regressor}\")\n",
    "    dataframes = []\n",
    "    for (job, (train, test)) in datasets_split.items():\n",
    "        print(f\"Comparing big regressor vs local regressor for job {job}\")\n",
    "        print(\"Preparing data for local regressor\")\n",
    "        joint_df = pd.concat([train, test])\n",
    "#         local_X, local_y = get_numerical_pipeline_data_big(joint_df, exp4_resources)\n",
    "#         print(f\"{len(local_X)} {len(train.index)} {len(test.index)}\")\n",
    "#         local_X_train, local_X_test, local_y_train, local_y_test = local_X.loc[train.index], local_X.loc[test.index], local_y.loc[train.index], local_y.loc[test.index]\n",
    "        \n",
    "#         print(f\"Rating local regressors for job {job}\")\n",
    "#         regressor_df = rate_data_explicit(local_X_train, local_X_test, local_y_train, local_y_test, basic_regressors, verbose=0)\n",
    "#         print(regressor_df.head())\n",
    "        print(\"Preparing data for big regressor\")\n",
    "        local_X_test, local_y_test = X.loc[test.index], y.loc[test.index]\n",
    "        \n",
    "        print(f\"Rating big regressor for job {job}\")\n",
    "        local_predicted = big_regressor.predict(local_X_test)\n",
    "        [r2, mae, mape, rae_score] = calculate_regression_score(local_predicted, local_y_test, [r2_score, mean_absolute_error, mean_absolute_percentage_error, rae])\n",
    "        adjusted_r2 = get_adjusted_r2(r2, local_y_test, local_X_test)\n",
    "        dataframes.append({\"name\": \"big\", \"job\": job, **{\"r2\": r2, \"mae\": mae, \"mape\": mape, \"rae\": rae_score, \"adjusted_r2\": adjusted_r2}, \"pca\": \"xD\"})\n",
    "        \n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment4_numerical_rae():\n",
    "    exp4_resources = [\"read_sum\", \"write_sum\", \"cpu_max\", \"cpu_mean\", \"memory_mean\", \"memory_max\"]\n",
    "        \n",
    "    \n",
    "    train_indices = np.concatenate([train.index for (_, (train, _)) in datasets_split.items()])\n",
    "    test_indices = np.concatenate([test.index for (_, (_, test)) in datasets_split.items()])\n",
    "    dataframes = []\n",
    "    for (job, (train, test)) in datasets_split.items():\n",
    "        print(\"Preparing data for local regressor\")\n",
    "        joint_df = pd.concat([train, test])\n",
    "        local_X, local_y = get_numerical_pipeline_data_big(joint_df, exp4_resources)\n",
    "        print(f\"{len(local_X)} {len(train.index)} {len(test.index)}\")\n",
    "        local_X_train, local_X_test, local_y_train, local_y_test = local_X.loc[train.index], local_X.loc[test.index], local_y.loc[train.index], local_y.loc[test.index]\n",
    "        \n",
    "        print(f\"Rating local regressors for job {job}\")\n",
    "        regressor_df = rate_data_explicit(local_X_train, local_X_test, local_y_train, local_y_test, basic_regressors, verbose=0, scoring=rae_scorer)\n",
    "        print(regressor_df.head())\n",
    "        regressor_df[\"job\"] = job\n",
    "        regressor_df[\"size\"] = len(joint_df)\n",
    "        dataframes.append(regressor_df)\n",
    "        \n",
    "    return pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_run(file, runner):\n",
    "    if not os.path.isfile(file):\n",
    "        print(f\"Running experiment {file}\")\n",
    "        dataframe = runner()\n",
    "        dataframe.to_csv(file)\n",
    "    else:\n",
    "        dataframe = pd.read_csv(file).round(2)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_JOBS=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp4_r2_df_incomplete = load_or_run(\"data/exp4_optimize_r2.csv\", run_experiment4)\n",
    "exp4_r2_df_incomplete = load_or_run(\"rerun4df.csv\", lambda: None)\n",
    "# exp4_rae_incomplete = load_or_run(\"data/exp4_optimize_rae.csv\", run_experiment4_numerical_rae)\n",
    "# exp4_r2_df_memory_intensive = load_or_run(\"data/exp4_optimize_r2_memint.csv\", run_experiment4_memory_intensive)\n",
    "# exp4_r2_categorical_incomplete = load_or_run(\"data/exp4_optimize_r2_categorical.csv\", run_experiment4_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>name</th>\n",
       "      <th>pca</th>\n",
       "      <th>adjusted_r2</th>\n",
       "      <th>r2</th>\n",
       "      <th>mae</th>\n",
       "      <th>mape</th>\n",
       "      <th>best_score</th>\n",
       "      <th>params</th>\n",
       "      <th>rae</th>\n",
       "      <th>job</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>knn</td>\n",
       "      <td>26</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.33</td>\n",
       "      <td>83.25</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.40</td>\n",
       "      <td>{'knn__n_neighbors': 1, 'pca__n_components': 26}</td>\n",
       "      <td>0.68</td>\n",
       "      <td>add_replace</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>dtr</td>\n",
       "      <td>34</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.12</td>\n",
       "      <td>98.26</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.55</td>\n",
       "      <td>{'dtr__criterion': 'friedman_mse', 'dtr__max_d...</td>\n",
       "      <td>0.80</td>\n",
       "      <td>add_replace</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>lasso</td>\n",
       "      <td>39</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.35</td>\n",
       "      <td>87.36</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.35</td>\n",
       "      <td>{'lasso__alpha': 0.9600000000000001, 'pca__n_c...</td>\n",
       "      <td>0.71</td>\n",
       "      <td>add_replace</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>mlp</td>\n",
       "      <td>25</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.27</td>\n",
       "      <td>99.12</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-1.56</td>\n",
       "      <td>{'mlp__activation': 'relu', 'mlp__hidden_layer...</td>\n",
       "      <td>0.80</td>\n",
       "      <td>add_replace</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>24</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.35</td>\n",
       "      <td>92.48</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.54</td>\n",
       "      <td>{'elasticnet__alpha': 0.41000000000000003, 'el...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>add_replace</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>2</td>\n",
       "      <td>lasso</td>\n",
       "      <td>39</td>\n",
       "      <td>-0.29</td>\n",
       "      <td>0.19</td>\n",
       "      <td>105.18</td>\n",
       "      <td>0.16</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>{'lasso__alpha': 0.9600000000000001, 'pca__n_c...</td>\n",
       "      <td>0.74</td>\n",
       "      <td>sort_sam</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>3</td>\n",
       "      <td>mlp</td>\n",
       "      <td>25</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.26</td>\n",
       "      <td>110.06</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-1.43</td>\n",
       "      <td>{'mlp__activation': 'relu', 'mlp__hidden_layer...</td>\n",
       "      <td>0.77</td>\n",
       "      <td>sort_sam</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>4</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>38</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>0.16</td>\n",
       "      <td>106.29</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.27</td>\n",
       "      <td>{'elasticnet__alpha': 0.51, 'elasticnet__l1_ra...</td>\n",
       "      <td>0.74</td>\n",
       "      <td>sort_sam</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>5</td>\n",
       "      <td>svr</td>\n",
       "      <td>20</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>0.18</td>\n",
       "      <td>103.87</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.24</td>\n",
       "      <td>{'pca__n_components': 20, 'svr__alpha': 0.1901...</td>\n",
       "      <td>0.73</td>\n",
       "      <td>sort_sam</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>6</td>\n",
       "      <td>big</td>\n",
       "      <td>xD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.76</td>\n",
       "      <td>947.62</td>\n",
       "      <td>0.24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.13</td>\n",
       "      <td>sort_sam</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>196 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0        name pca  adjusted_r2    r2     mae  mape  best_score  \\\n",
       "0             0         knn  26        -0.06  0.33   83.25  0.12        0.40   \n",
       "1             1         dtr  34        -0.40  0.12   98.26  0.15       -0.55   \n",
       "2             2       lasso  39        -0.04  0.35   87.36  0.14        0.35   \n",
       "3             3         mlp  25        -0.15  0.27   99.12  0.16       -1.56   \n",
       "4             4  elasticnet  24        -0.03  0.35   92.48  0.15        0.54   \n",
       "..          ...         ...  ..          ...   ...     ...   ...         ...   \n",
       "191           2       lasso  39        -0.29  0.19  105.18  0.16       -0.09   \n",
       "192           3         mlp  25        -0.18  0.26  110.06  0.17       -1.43   \n",
       "193           4  elasticnet  38        -0.34  0.16  106.29  0.16        0.27   \n",
       "194           5         svr  20        -0.31  0.18  103.87  0.15        0.24   \n",
       "195           6         big  xD          NaN  0.76  947.62  0.24         NaN   \n",
       "\n",
       "                                                params   rae          job  \\\n",
       "0     {'knn__n_neighbors': 1, 'pca__n_components': 26}  0.68  add_replace   \n",
       "1    {'dtr__criterion': 'friedman_mse', 'dtr__max_d...  0.80  add_replace   \n",
       "2    {'lasso__alpha': 0.9600000000000001, 'pca__n_c...  0.71  add_replace   \n",
       "3    {'mlp__activation': 'relu', 'mlp__hidden_layer...  0.80  add_replace   \n",
       "4    {'elasticnet__alpha': 0.41000000000000003, 'el...  0.75  add_replace   \n",
       "..                                                 ...   ...          ...   \n",
       "191  {'lasso__alpha': 0.9600000000000001, 'pca__n_c...  0.74     sort_sam   \n",
       "192  {'mlp__activation': 'relu', 'mlp__hidden_layer...  0.77     sort_sam   \n",
       "193  {'elasticnet__alpha': 0.51, 'elasticnet__l1_ra...  0.74     sort_sam   \n",
       "194  {'pca__n_components': 20, 'svr__alpha': 0.1901...  0.73     sort_sam   \n",
       "195                                                NaN  0.13     sort_sam   \n",
       "\n",
       "     size  \n",
       "0     427  \n",
       "1     427  \n",
       "2     427  \n",
       "3     427  \n",
       "4     427  \n",
       "..    ...  \n",
       "191   427  \n",
       "192   427  \n",
       "193   427  \n",
       "194   427  \n",
       "195   427  \n",
       "\n",
       "[196 rows x 12 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp4_r2_df_incomplete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for big regressor\n",
      "111329 111329 83489 27840\n",
      "Making split with test as 0.25006961348795015 of dataset\n",
      "Training big regressor with train data\n",
      "[Pipeline] ............... (step 1 of 2) Processing pca, total=   1.4s\n"
     ]
    }
   ],
   "source": [
    "big_regressors_adjusteds = run_experiment4_adjustedr2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_df = exp4_r2_df_incomplete.drop(columns=[\"Unnamed: 0\"])\n",
    "comparison_rows = []\n",
    "for job in raw_datasets:\n",
    "    big = table_df.loc[(table_df.job == job) & (table_df.name == \"big\")].iloc[0]\n",
    "    top_small = table_df.loc[(table_df.job == job) & ~(table_df.name == \"big\")].sort_values(\"r2\", ascending=False).iloc[0]\n",
    "    r2diff = big[\"r2\"] - top_small[\"r2\"]\n",
    "    mapediff = big[\"mape\"] - top_small[\"mape\"]\n",
    "    comparison_rows.append({\n",
    "     \"job\": job,\n",
    "     \"size\": big[\"size\"],\n",
    "     \"big_components\": 71,\n",
    "     \"small_components\": top_small[\"pca\"],\n",
    "     \"big_r2\": big[\"r2\"],\n",
    "     \"small_r2\": top_small[\"r2\"],\n",
    "     \"big_mape\": big[\"mape\"],\n",
    "     \"small_mape\": top_small[\"mape\"],\n",
    "     \"bigminsmallr2\": r2diff,\n",
    "     \"bigminsmallmape\": mapediff,\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_rows)\n",
    "comparison_df.to_csv(\"comparisonexp2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_for_symbolic_regression():\n",
    "    exp4_resources = [\"read_sum\", \"write_sum\", \"cpu_max\", \"cpu_mean\", \"memory_mean\", \"memory_max\"]\n",
    "            \n",
    "    train_indices = np.concatenate([train.index for (_, (train, _)) in datasets_split.items()])\n",
    "    test_indices = np.concatenate([test.index for (_, (_, test)) in datasets_split.items()])\n",
    "    for (job, (train, test)) in datasets_split.items():\n",
    "        print(f\"Preparing data for symbolic regressor: {job}, {len(train) + len(test)}\")\n",
    "        joint_df = pd.concat([train, test])\n",
    "        local_X, local_y = get_symbolic_regression_data(joint_df, exp4_resources)\n",
    "        local_X_train, local_X_test, local_y_train, local_y_test = local_X.loc[train.index], local_X.loc[test.index], local_y.loc[train.index], local_y.loc[test.index]\n",
    "        \n",
    "        joint_train = local_X_train.join(local_y_train)\n",
    "        joint_test = local_X_test.join(local_y_test)\n",
    "\n",
    "        joint_train.to_csv(f\"data/symbolic/{job}_train.csv\", sep=\"\\t\", index=False)\n",
    "        joint_test.to_csv(f\"data/symbolic/{job}_test.csv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_for_symbolic_regression_single_step():\n",
    "    exp4_resources = []\n",
    "            \n",
    "    train_indices = np.concatenate([train.index for (_, (train, _)) in datasets_split.items()])\n",
    "    test_indices = np.concatenate([test.index for (_, (_, test)) in datasets_split.items()])\n",
    "    for (job, (train, test)) in datasets_split.items():\n",
    "        print(f\"Preparing data for symbolic regressor: {job}, {len(train) + len(test)}\")\n",
    "        joint_df = pd.concat([train, test])\n",
    "        local_X, local_y = get_symbolic_regression_data(joint_df, exp4_resources)\n",
    "        local_X_train, local_X_test, local_y_train, local_y_test = local_X.loc[train.index], local_X.loc[test.index], local_y.loc[train.index], local_y.loc[test.index]\n",
    "        \n",
    "        joint_train = local_X_train.join(local_y_train)\n",
    "        joint_test = local_X_test.join(local_y_test)\n",
    "\n",
    "        joint_train.to_csv(f\"data/symbolicRaw/{job}_train.csv\", sep=\"\\t\", index=False)\n",
    "        joint_test.to_csv(f\"data/symbolicRaw/{job}_test.csv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for symbolic regressor: add_replace, 427\n",
      "Preparing data for symbolic regressor: alignment_to_reference, 427\n",
      "Preparing data for symbolic regressor: bwa-index, 63\n",
      "Preparing data for symbolic regressor: combine_variants, 62\n",
      "Preparing data for symbolic regressor: dedup, 427\n",
      "Preparing data for symbolic regressor: faidx, 63\n",
      "Preparing data for symbolic regressor: filtering_indel, 62\n",
      "Preparing data for symbolic regressor: filtering_snp, 62\n",
      "Preparing data for symbolic regressor: genotype_gvcfs, 1259\n",
      "Preparing data for symbolic regressor: haplotype_caller, 8539\n",
      "Preparing data for symbolic regressor: indel_realign, 427\n",
      "Preparing data for symbolic regressor: mAdd, 160\n",
      "Preparing data for symbolic regressor: mBackground, 15340\n",
      "Preparing data for symbolic regressor: mBgModel, 160\n",
      "Preparing data for symbolic regressor: mConcatFit, 160\n",
      "Preparing data for symbolic regressor: mDiffFit, 65323\n",
      "Preparing data for symbolic regressor: mImgtbl, 157\n",
      "Preparing data for symbolic regressor: mJPEG, 43\n",
      "Preparing data for symbolic regressor: mProject, 11316\n",
      "Preparing data for symbolic regressor: mProjectPP, 5551\n",
      "Preparing data for symbolic regressor: mShrink, 43\n",
      "Preparing data for symbolic regressor: mViewer, 156\n",
      "Preparing data for symbolic regressor: merge_gcvf, 61\n",
      "Preparing data for symbolic regressor: realign_target_creator, 427\n",
      "Preparing data for symbolic regressor: select_variants_indel, 62\n",
      "Preparing data for symbolic regressor: select_variants_snp, 62\n",
      "Preparing data for symbolic regressor: seq_dict, 63\n",
      "Preparing data for symbolic regressor: sort_sam, 427\n"
     ]
    }
   ],
   "source": [
    "export_for_symbolic_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for symbolic regressor: add_replace, 427\n",
      "Preparing data for symbolic regressor: alignment_to_reference, 427\n",
      "Preparing data for symbolic regressor: bwa-index, 63\n",
      "Preparing data for symbolic regressor: combine_variants, 62\n",
      "Preparing data for symbolic regressor: dedup, 427\n",
      "Preparing data for symbolic regressor: faidx, 63\n",
      "Preparing data for symbolic regressor: filtering_indel, 62\n",
      "Preparing data for symbolic regressor: filtering_snp, 62\n",
      "Preparing data for symbolic regressor: genotype_gvcfs, 1259\n",
      "Preparing data for symbolic regressor: haplotype_caller, 8539\n",
      "Preparing data for symbolic regressor: indel_realign, 427\n",
      "Preparing data for symbolic regressor: mAdd, 160\n",
      "Preparing data for symbolic regressor: mBackground, 15340\n",
      "Preparing data for symbolic regressor: mBgModel, 160\n",
      "Preparing data for symbolic regressor: mConcatFit, 160\n",
      "Preparing data for symbolic regressor: mDiffFit, 65323\n",
      "Preparing data for symbolic regressor: mImgtbl, 157\n",
      "Preparing data for symbolic regressor: mJPEG, 43\n",
      "Preparing data for symbolic regressor: mProject, 11316\n",
      "Preparing data for symbolic regressor: mProjectPP, 5551\n",
      "Preparing data for symbolic regressor: mShrink, 43\n",
      "Preparing data for symbolic regressor: mViewer, 156\n",
      "Preparing data for symbolic regressor: merge_gcvf, 61\n",
      "Preparing data for symbolic regressor: realign_target_creator, 427\n",
      "Preparing data for symbolic regressor: select_variants_indel, 62\n",
      "Preparing data for symbolic regressor: select_variants_snp, 62\n",
      "Preparing data for symbolic regressor: seq_dict, 63\n",
      "Preparing data for symbolic regressor: sort_sam, 427\n"
     ]
    }
   ],
   "source": [
    "export_for_symbolic_regression_single_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tworzenie modeli zwróconych przez regresję symboliczną"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr_models = {\n",
    "    \"add_replace\": lambda row: 1309 + 6.299e-8* row[\"mem.active\"] -2.82e-9 * row[\"total_ram_available\"] - 259.6 * row[\"cpu.speed\"],\n",
    "    \"alignment_to_reference\": lambda row: 1095 + 2291*row[\"cpu_mean\"] + 0.882 * math.exp(10.1 * row[\"read_sum\"]),\n",
    "    \"bwa-index\": lambda row: 862001 + 6.991e5 * row[\"write_sum\"] + 2.62e-5 * row[\"mem.slab\"],\n",
    "    \"combine_variants\": lambda row: 10816 - 6.96 * row[\"avg_pods\"] - 470 * row[\"avg_cpus\"] - 1532 * row[\"cpu.speed\"],\n",
    "    \"dedup\": lambda row: 5367 + 1713*math.tan(1.82*row[\"cpu_mean\"] - 0.464) - 2524*row[\"cpu_max\"],\n",
    "    \"faidx\": lambda row: 4035 + 1887*row[\"write_sum\"] + 1.26**row[\"read_sum\"],\n",
    "    \"filtering_indel\": lambda row: 7525  - 9.58e-8*row[\"average_ram_available\"]-1133*row[\"cpu.speed\"],\n",
    "    \"filtering_snp\": lambda row: 5099 + 6.10e-7 * row[\"mem.slab\"] - 226 * row[\"cpu.speed\"] * row[\"avg_cpus\"],\n",
    "    \"genotype_gvcfs\": lambda row: 5.25e4 + 16562*row[\"args\"]*row[\"read_sum\"] - 772 * row[\"size\"],\n",
    "    \"haplotype_caller\": lambda row: 4.166e4 + 2.8e4 * row[\"read_sum\"]**2,\n",
    "    \"indel_realign\": lambda row: 7172 + 1065*row[\"cpu_mean\"] - 1218 * row[\"cpu.speed\"] - 1879 * row[\"cpu_max\"],\n",
    "    \"mAdd\": lambda row: 25893 * row[\"write_sum\"] + 478 * (1 if row[\"workflowName\"] == \"montage2\" else 0) * row[\"inputs\"] * (1 if row[\"cpu.brand\"] == \"epyc\" else 0) * row[\"write_sum\"] - 7825,\n",
    "    \"mBackground\": lambda row: 1187 * (1 if row[\"workflowName\"] == \"montage2\" else 0) + row[\"write_sum\"] * math.factorial(int(8.12 * row[\"read_sum\"])),\n",
    "    \"mBgModel\": lambda row: 4.65e4 * row[\"read_sum\"] * row[\"memory_mean\"] + 3.4e4 * row[\"size\"] * row[\"write_sum\"] - 2.8e4 * row[\"cpu_mean\"],\n",
    "    \"mConcatFit\": lambda row: 9.71 * row[\"inputs\"] + 0.805 * (1.34e5) ** row[\"write_sum\"],\n",
    "    \"mDiffFit\": lambda row: 3.82e5 * row[\"cpu_max\"] - 1.25e5 - row[\"avg_pods\"] - 2.52e5 * row[\"cpu_mean\"] ** 2,\n",
    "    \"merge_gcvf\": lambda row: 1.15e6 + 1.89e4 * row[\"inputs\"] * row[\"read_sum\"] + 1.91e7 * row[\"write_sum\"] ** 2,\n",
    "    \"mImgtbl\": lambda row: 149 - (183 / math.log(row[\"read_sum\"] * 1.001)) ,\n",
    "    \"mJPEG\": lambda row: 733 - 34 * row[\"cpu.speed\"]** 2,\n",
    "    \"mProject\": lambda row: 12324 + 1.68e4 * row[\"read_sum\"] + 1.23e4 ** row[\"read_sum\"],\n",
    "    \"mProjectPP\": lambda row: 3102 * row[\"write_sum\"] + 2.37e-8 * row[\"mem.total\"] - 409 - 80.64 * row[\"cpu.speed\"] * row[\"avg_cpus\"],\n",
    "    \"mShrink\": lambda row: 67.5 + 500 * row[\"size\"] * row[\"write_sum\"] + 272 * row[\"size\"] ** 2,\n",
    "    \"mViewer\": lambda row: 2.55e4*row[\"write_sum\"] - 8373,\n",
    "    \"realign_target_creator\": lambda row: 8.19e4 + 2.26e5 * row[\"write_sum\"] ** 2,\n",
    "    \"select_variants_indel\": lambda row: 2.173e4 + 8.85e4 * row[\"read_sum\"] - 3.84e4 * row[\"write_sum\"],\n",
    "    \"select_variants_snp\": lambda row: 2.29e4+5.01e4*row[\"read_sum\"],\n",
    "    \"seq_dict\": lambda row: 6.37e3 + (33.9 * row[\"total_cpus\"]*row[\"write_sum\"]) / row[\"cpu_mean\"],\n",
    "    \"sort_sam\": lambda row: 1069 + 1.87 * row[\"avg_pods\"] - 205 * row[\"cpu.speed\"] - 3.35e-11 * row[\"avg_pods\"] * row[\"total_ram_available\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n(series, value):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(series.to_numpy().reshape(-1, 1))\n",
    "    return scaler.transform(np.array([value]).reshape(1, -1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dataset(job, train, test, resources):\n",
    "    print(f\"Evaluating 0/1: {job}, {len(train) + len(test)}\")\n",
    "    joint_df = pd.concat([train, test])\n",
    "    local_X, local_y = get_symbolic_regression_data(joint_df, resources)\n",
    "    local_X_train, local_X_test, local_y_train, local_y_test = local_X.loc[train.index], local_X.loc[test.index], local_y.loc[train.index], local_y.loc[test.index]\n",
    "\n",
    "    y_pred = np.array([sr_models[job](row) for index, row in local_X_test.iterrows()])\n",
    "    y_true = local_y_test\n",
    "    r2, mae, mape, rae = calculate_regression_score(y_true, y_pred)\n",
    "    adjusted_r2 = 1 - (1-r2)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)\n",
    "    print(f\"Evaluating 1/1: {job}\")\n",
    "    return {\"name\": \"symbolic\", \"pca\": None, \"r2\": r2, \"adjusted_r2\": adjusted_r2, \"mae\": mae, \"mape\": mape, \"rae\": rae,\"best_score\": r2, \"params\": None, \"job\": job, \"size\": len(joint_df)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_all_d(true, pred, scores=[r2_score, mean_absolute_error, mean_absolute_percentage_error, rae]):\n",
    "    executor = get_reusable_executor(max_workers=4)\n",
    "    results = executor.map(lambda fun: fun(true, pred), scores)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_symbolic_regression():\n",
    "    exp4_resources = [\"read_sum\", \"write_sum\", \"cpu_max\", \"cpu_mean\", \"memory_mean\", \"memory_max\"]\n",
    "    results_df = pd.DataFrame(columns = ['name', 'pca', 'adjusted_r2', 'r2', 'mae', 'mape','best_score', 'params', 'rae', 'job', 'size'])\n",
    "    executor = get_reusable_executor(max_workers=12)\n",
    "    results = executor.map(lambda item: eval_dataset(item[0], item[1][0], item[1][1], exp4_resources), datasets_split.items())\n",
    "    for result in results:\n",
    "        results_df = results_df.append(result, ignore_index=True)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbolic_results = evaluate_symbolic_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>pca</th>\n",
       "      <th>adjusted_r2</th>\n",
       "      <th>r2</th>\n",
       "      <th>mae</th>\n",
       "      <th>mape</th>\n",
       "      <th>best_score</th>\n",
       "      <th>params</th>\n",
       "      <th>rae</th>\n",
       "      <th>job</th>\n",
       "      <th>size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>symbolic</td>\n",
       "      <td>None</td>\n",
       "      <td>0.343227</td>\n",
       "      <td>0.344112</td>\n",
       "      <td>91.706931</td>\n",
       "      <td>0.150273</td>\n",
       "      <td>0.344112</td>\n",
       "      <td>None</td>\n",
       "      <td>0.744599</td>\n",
       "      <td>add_replace</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>symbolic</td>\n",
       "      <td>None</td>\n",
       "      <td>0.936313</td>\n",
       "      <td>0.936398</td>\n",
       "      <td>710.318725</td>\n",
       "      <td>0.300008</td>\n",
       "      <td>0.936398</td>\n",
       "      <td>None</td>\n",
       "      <td>0.298720</td>\n",
       "      <td>alignment_to_reference</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>symbolic</td>\n",
       "      <td>None</td>\n",
       "      <td>0.941722</td>\n",
       "      <td>0.941800</td>\n",
       "      <td>36311.687201</td>\n",
       "      <td>0.028410</td>\n",
       "      <td>0.941800</td>\n",
       "      <td>None</td>\n",
       "      <td>0.193963</td>\n",
       "      <td>bwa-index</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>symbolic</td>\n",
       "      <td>None</td>\n",
       "      <td>0.578988</td>\n",
       "      <td>0.579555</td>\n",
       "      <td>382.321979</td>\n",
       "      <td>0.090921</td>\n",
       "      <td>0.579555</td>\n",
       "      <td>None</td>\n",
       "      <td>0.640020</td>\n",
       "      <td>combine_variants</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>symbolic</td>\n",
       "      <td>None</td>\n",
       "      <td>0.853968</td>\n",
       "      <td>0.854165</td>\n",
       "      <td>560.087663</td>\n",
       "      <td>0.106819</td>\n",
       "      <td>0.854165</td>\n",
       "      <td>None</td>\n",
       "      <td>0.402728</td>\n",
       "      <td>dedup</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>symbolic</td>\n",
       "      <td>None</td>\n",
       "      <td>0.301691</td>\n",
       "      <td>0.302632</td>\n",
       "      <td>1144.073241</td>\n",
       "      <td>0.196323</td>\n",
       "      <td>0.302632</td>\n",
       "      <td>None</td>\n",
       "      <td>0.729370</td>\n",
       "      <td>faidx</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>symbolic</td>\n",
       "      <td>None</td>\n",
       "      <td>0.573143</td>\n",
       "      <td>0.573718</td>\n",
       "      <td>292.266786</td>\n",
       "      <td>0.082379</td>\n",
       "      <td>0.573718</td>\n",
       "      <td>None</td>\n",
       "      <td>0.553085</td>\n",
       "      <td>filtering_indel</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>symbolic</td>\n",
       "      <td>None</td>\n",
       "      <td>0.834844</td>\n",
       "      <td>0.835066</td>\n",
       "      <td>177.792762</td>\n",
       "      <td>0.050664</td>\n",
       "      <td>0.835066</td>\n",
       "      <td>None</td>\n",
       "      <td>0.336808</td>\n",
       "      <td>filtering_snp</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>symbolic</td>\n",
       "      <td>None</td>\n",
       "      <td>0.985947</td>\n",
       "      <td>0.985966</td>\n",
       "      <td>9106.196608</td>\n",
       "      <td>0.121102</td>\n",
       "      <td>0.985966</td>\n",
       "      <td>None</td>\n",
       "      <td>0.096878</td>\n",
       "      <td>genotype_gvcfs</td>\n",
       "      <td>1259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>symbolic</td>\n",
       "      <td>None</td>\n",
       "      <td>0.819421</td>\n",
       "      <td>0.819664</td>\n",
       "      <td>3251.689265</td>\n",
       "      <td>0.064657</td>\n",
       "      <td>0.819664</td>\n",
       "      <td>None</td>\n",
       "      <td>0.378582</td>\n",
       "      <td>haplotype_caller</td>\n",
       "      <td>8539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>symbolic</td>\n",
       "      <td>None</td>\n",
       "      <td>0.733589</td>\n",
       "      <td>0.733948</td>\n",
       "      <td>295.151106</td>\n",
       "      <td>0.076799</td>\n",
       "      <td>0.733948</td>\n",
       "      <td>None</td>\n",
       "      <td>0.498781</td>\n",
       "      <td>indel_realign</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>symbolic</td>\n",
       "      <td>None</td>\n",
       "      <td>0.016040</td>\n",
       "      <td>0.017366</td>\n",
       "      <td>20532.193125</td>\n",
       "      <td>1.176430</td>\n",
       "      <td>0.017366</td>\n",
       "      <td>None</td>\n",
       "      <td>0.588218</td>\n",
       "      <td>mAdd</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>symbolic</td>\n",
       "      <td>None</td>\n",
       "      <td>0.514166</td>\n",
       "      <td>0.514821</td>\n",
       "      <td>1930.223996</td>\n",
       "      <td>1.186593</td>\n",
       "      <td>0.514821</td>\n",
       "      <td>None</td>\n",
       "      <td>0.436231</td>\n",
       "      <td>mBackground</td>\n",
       "      <td>15340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>symbolic</td>\n",
       "      <td>None</td>\n",
       "      <td>0.979584</td>\n",
       "      <td>0.979611</td>\n",
       "      <td>2203.123926</td>\n",
       "      <td>1.728779</td>\n",
       "      <td>0.979611</td>\n",
       "      <td>None</td>\n",
       "      <td>0.131944</td>\n",
       "      <td>mBgModel</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>symbolic</td>\n",
       "      <td>None</td>\n",
       "      <td>0.989695</td>\n",
       "      <td>0.989709</td>\n",
       "      <td>1249.889357</td>\n",
       "      <td>0.333214</td>\n",
       "      <td>0.989709</td>\n",
       "      <td>None</td>\n",
       "      <td>0.082635</td>\n",
       "      <td>mConcatFit</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>symbolic</td>\n",
       "      <td>None</td>\n",
       "      <td>0.587349</td>\n",
       "      <td>0.587905</td>\n",
       "      <td>669.611119</td>\n",
       "      <td>3.434186</td>\n",
       "      <td>0.587905</td>\n",
       "      <td>None</td>\n",
       "      <td>0.799779</td>\n",
       "      <td>mDiffFit</td>\n",
       "      <td>65323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>symbolic</td>\n",
       "      <td>None</td>\n",
       "      <td>-1.334140</td>\n",
       "      <td>-1.330994</td>\n",
       "      <td>14110.610051</td>\n",
       "      <td>3.305431</td>\n",
       "      <td>-1.330994</td>\n",
       "      <td>None</td>\n",
       "      <td>0.800541</td>\n",
       "      <td>mImgtbl</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>symbolic</td>\n",
       "      <td>None</td>\n",
       "      <td>0.729328</td>\n",
       "      <td>0.729693</td>\n",
       "      <td>28.455909</td>\n",
       "      <td>0.057311</td>\n",
       "      <td>0.729693</td>\n",
       "      <td>None</td>\n",
       "      <td>0.561874</td>\n",
       "      <td>mJPEG</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>symbolic</td>\n",
       "      <td>None</td>\n",
       "      <td>0.954676</td>\n",
       "      <td>0.954737</td>\n",
       "      <td>1183.387820</td>\n",
       "      <td>0.066820</td>\n",
       "      <td>0.954737</td>\n",
       "      <td>None</td>\n",
       "      <td>0.192147</td>\n",
       "      <td>mProject</td>\n",
       "      <td>11316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>symbolic</td>\n",
       "      <td>None</td>\n",
       "      <td>0.613277</td>\n",
       "      <td>0.613798</td>\n",
       "      <td>113.614392</td>\n",
       "      <td>0.130380</td>\n",
       "      <td>0.613798</td>\n",
       "      <td>None</td>\n",
       "      <td>0.705175</td>\n",
       "      <td>mProjectPP</td>\n",
       "      <td>5551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>symbolic</td>\n",
       "      <td>None</td>\n",
       "      <td>0.973666</td>\n",
       "      <td>0.973701</td>\n",
       "      <td>68.984144</td>\n",
       "      <td>0.295288</td>\n",
       "      <td>0.973701</td>\n",
       "      <td>None</td>\n",
       "      <td>0.183388</td>\n",
       "      <td>mShrink</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>symbolic</td>\n",
       "      <td>None</td>\n",
       "      <td>0.806797</td>\n",
       "      <td>0.807057</td>\n",
       "      <td>1488.723866</td>\n",
       "      <td>0.553356</td>\n",
       "      <td>0.807057</td>\n",
       "      <td>None</td>\n",
       "      <td>0.290954</td>\n",
       "      <td>mViewer</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>symbolic</td>\n",
       "      <td>None</td>\n",
       "      <td>0.983696</td>\n",
       "      <td>0.983718</td>\n",
       "      <td>896495.825047</td>\n",
       "      <td>0.082279</td>\n",
       "      <td>0.983718</td>\n",
       "      <td>None</td>\n",
       "      <td>0.115395</td>\n",
       "      <td>merge_gcvf</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>symbolic</td>\n",
       "      <td>None</td>\n",
       "      <td>0.782852</td>\n",
       "      <td>0.783145</td>\n",
       "      <td>10739.242684</td>\n",
       "      <td>0.048687</td>\n",
       "      <td>0.783145</td>\n",
       "      <td>None</td>\n",
       "      <td>0.185756</td>\n",
       "      <td>realign_target_creator</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>symbolic</td>\n",
       "      <td>None</td>\n",
       "      <td>-0.017876</td>\n",
       "      <td>-0.016505</td>\n",
       "      <td>78451.949597</td>\n",
       "      <td>0.158019</td>\n",
       "      <td>-0.016505</td>\n",
       "      <td>None</td>\n",
       "      <td>0.558479</td>\n",
       "      <td>select_variants_indel</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>symbolic</td>\n",
       "      <td>None</td>\n",
       "      <td>-0.016692</td>\n",
       "      <td>-0.015322</td>\n",
       "      <td>77293.286290</td>\n",
       "      <td>0.134289</td>\n",
       "      <td>-0.015322</td>\n",
       "      <td>None</td>\n",
       "      <td>0.548815</td>\n",
       "      <td>select_variants_snp</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>symbolic</td>\n",
       "      <td>None</td>\n",
       "      <td>0.755405</td>\n",
       "      <td>0.755734</td>\n",
       "      <td>804.483758</td>\n",
       "      <td>0.132876</td>\n",
       "      <td>0.755734</td>\n",
       "      <td>None</td>\n",
       "      <td>0.538930</td>\n",
       "      <td>seq_dict</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>symbolic</td>\n",
       "      <td>None</td>\n",
       "      <td>0.148688</td>\n",
       "      <td>0.149835</td>\n",
       "      <td>116.073326</td>\n",
       "      <td>0.178395</td>\n",
       "      <td>0.149835</td>\n",
       "      <td>None</td>\n",
       "      <td>0.812528</td>\n",
       "      <td>sort_sam</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        name   pca  adjusted_r2        r2            mae      mape  \\\n",
       "0   symbolic  None     0.343227  0.344112      91.706931  0.150273   \n",
       "1   symbolic  None     0.936313  0.936398     710.318725  0.300008   \n",
       "2   symbolic  None     0.941722  0.941800   36311.687201  0.028410   \n",
       "3   symbolic  None     0.578988  0.579555     382.321979  0.090921   \n",
       "4   symbolic  None     0.853968  0.854165     560.087663  0.106819   \n",
       "5   symbolic  None     0.301691  0.302632    1144.073241  0.196323   \n",
       "6   symbolic  None     0.573143  0.573718     292.266786  0.082379   \n",
       "7   symbolic  None     0.834844  0.835066     177.792762  0.050664   \n",
       "8   symbolic  None     0.985947  0.985966    9106.196608  0.121102   \n",
       "9   symbolic  None     0.819421  0.819664    3251.689265  0.064657   \n",
       "10  symbolic  None     0.733589  0.733948     295.151106  0.076799   \n",
       "11  symbolic  None     0.016040  0.017366   20532.193125  1.176430   \n",
       "12  symbolic  None     0.514166  0.514821    1930.223996  1.186593   \n",
       "13  symbolic  None     0.979584  0.979611    2203.123926  1.728779   \n",
       "14  symbolic  None     0.989695  0.989709    1249.889357  0.333214   \n",
       "15  symbolic  None     0.587349  0.587905     669.611119  3.434186   \n",
       "16  symbolic  None    -1.334140 -1.330994   14110.610051  3.305431   \n",
       "17  symbolic  None     0.729328  0.729693      28.455909  0.057311   \n",
       "18  symbolic  None     0.954676  0.954737    1183.387820  0.066820   \n",
       "19  symbolic  None     0.613277  0.613798     113.614392  0.130380   \n",
       "20  symbolic  None     0.973666  0.973701      68.984144  0.295288   \n",
       "21  symbolic  None     0.806797  0.807057    1488.723866  0.553356   \n",
       "22  symbolic  None     0.983696  0.983718  896495.825047  0.082279   \n",
       "23  symbolic  None     0.782852  0.783145   10739.242684  0.048687   \n",
       "24  symbolic  None    -0.017876 -0.016505   78451.949597  0.158019   \n",
       "25  symbolic  None    -0.016692 -0.015322   77293.286290  0.134289   \n",
       "26  symbolic  None     0.755405  0.755734     804.483758  0.132876   \n",
       "27  symbolic  None     0.148688  0.149835     116.073326  0.178395   \n",
       "\n",
       "    best_score params       rae                     job   size  \n",
       "0     0.344112   None  0.744599             add_replace    427  \n",
       "1     0.936398   None  0.298720  alignment_to_reference    427  \n",
       "2     0.941800   None  0.193963               bwa-index     63  \n",
       "3     0.579555   None  0.640020        combine_variants     62  \n",
       "4     0.854165   None  0.402728                   dedup    427  \n",
       "5     0.302632   None  0.729370                   faidx     63  \n",
       "6     0.573718   None  0.553085         filtering_indel     62  \n",
       "7     0.835066   None  0.336808           filtering_snp     62  \n",
       "8     0.985966   None  0.096878          genotype_gvcfs   1259  \n",
       "9     0.819664   None  0.378582        haplotype_caller   8539  \n",
       "10    0.733948   None  0.498781           indel_realign    427  \n",
       "11    0.017366   None  0.588218                    mAdd    160  \n",
       "12    0.514821   None  0.436231             mBackground  15340  \n",
       "13    0.979611   None  0.131944                mBgModel    160  \n",
       "14    0.989709   None  0.082635              mConcatFit    160  \n",
       "15    0.587905   None  0.799779                mDiffFit  65323  \n",
       "16   -1.330994   None  0.800541                 mImgtbl    157  \n",
       "17    0.729693   None  0.561874                   mJPEG     43  \n",
       "18    0.954737   None  0.192147                mProject  11316  \n",
       "19    0.613798   None  0.705175              mProjectPP   5551  \n",
       "20    0.973701   None  0.183388                 mShrink     43  \n",
       "21    0.807057   None  0.290954                 mViewer    156  \n",
       "22    0.983718   None  0.115395              merge_gcvf     61  \n",
       "23    0.783145   None  0.185756  realign_target_creator    427  \n",
       "24   -0.016505   None  0.558479   select_variants_indel     62  \n",
       "25   -0.015322   None  0.548815     select_variants_snp     62  \n",
       "26    0.755734   None  0.538930                seq_dict     63  \n",
       "27    0.149835   None  0.812528                sort_sam    427  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbolic_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_df = exp4_r2_df_incomplete.drop(columns=[\"Unnamed: 0\"])\n",
    "comparison_rows = []\n",
    "for job in raw_datasets:\n",
    "    big = table_df.loc[(table_df.job == job) & (table_df.name == \"big\")].sort_values(\"r2\", ascending=False).iloc[0]\n",
    "    classic = table_df.loc[(table_df.job == job) & ~(table_df.name == \"big\")].sort_values(\"r2\", ascending=False).iloc[0]\n",
    "    symbolic = symbolic_results.loc[symbolic_results.job == job].iloc[0]\n",
    "    comparison_rows.append({\n",
    "     \"job\": job,\n",
    "     \"size\": big[\"size\"],\n",
    "     \"big_r2\": big[\"r2\"],\n",
    "     \"big_mape\": big[\"mape\"],\n",
    "     \"classic_r2\": classic[\"r2\"],\n",
    "     \"classic_mape\": classic[\"mape\"],\n",
    "     \"symbolic_r2\": symbolic[\"r2\"],\n",
    "     \"symbolic_mape\": symbolic[\"mape\"],\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_rows)\n",
    "# comparison_df.to_csv(\"comparisonexp3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df.job = comparison_df.job.map(lambda name: name.replace(\"_\", \"\\_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df.symbolic_r2 = comparison_df.symbolic_r2.round(2)\n",
    "comparison_df.symbolic_mape = comparison_df.symbolic_mape.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df.to_csv(\"exp3Comparison.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job</th>\n",
       "      <th>size</th>\n",
       "      <th>big_r2</th>\n",
       "      <th>big_mape</th>\n",
       "      <th>classic_r2</th>\n",
       "      <th>classic_mape</th>\n",
       "      <th>symbolic_r2</th>\n",
       "      <th>symbolic_mape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alignment\\_to\\_reference</td>\n",
       "      <td>427</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bwa-index</td>\n",
       "      <td>63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>genotype\\_gvcfs</td>\n",
       "      <td>1259</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>mBgModel</td>\n",
       "      <td>160</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.93</td>\n",
       "      <td>2.21</td>\n",
       "      <td>0.98</td>\n",
       "      <td>1.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mConcatFit</td>\n",
       "      <td>160</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>mJPEG</td>\n",
       "      <td>43</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>merge\\_gcvf</td>\n",
       "      <td>61</td>\n",
       "      <td>0.90</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>select\\_variants\\_indel</td>\n",
       "      <td>62</td>\n",
       "      <td>-96.12</td>\n",
       "      <td>2.12</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         job  size  big_r2  big_mape  classic_r2  \\\n",
       "1   alignment\\_to\\_reference   427    0.82      0.22        0.89   \n",
       "2                  bwa-index    63    0.31      0.16        0.94   \n",
       "8            genotype\\_gvcfs  1259    0.96      0.15        0.97   \n",
       "13                  mBgModel   160    0.48      0.56        0.93   \n",
       "14                mConcatFit   160    0.87      0.65        0.81   \n",
       "17                     mJPEG    43    0.21      0.16        0.14   \n",
       "22               merge\\_gcvf    61    0.90      0.18        0.96   \n",
       "24   select\\_variants\\_indel    62  -96.12      2.12       -0.02   \n",
       "\n",
       "    classic_mape  symbolic_r2  symbolic_mape  \n",
       "1           0.19         0.94           0.30  \n",
       "2           0.04         0.94           0.03  \n",
       "8           0.11         0.99           0.12  \n",
       "13          2.21         0.98           1.73  \n",
       "14          1.17         0.99           0.33  \n",
       "17          0.08         0.73           0.06  \n",
       "22          0.19         0.98           0.08  \n",
       "24          0.14        -0.02           0.16  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = []\n",
    "for index, row in comparison_df.iterrows():\n",
    "    if row.big_r2 <= row.symbolic_r2 and row.classic_r2 <= row.symbolic_r2:\n",
    "        indices.append(row.name)\n",
    "    \n",
    "comparison_df.loc[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wyniki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykresy:\n",
    "- najlepsza skuteczność regresora (min mape/rae) vs liczba sampli\n",
    "- skuteczności 5 najlepszych regresorów dla każdego (grid chart)\n",
    "- najlepsza skuteczność regresora dla joba vs skuteczność dużego regresora dla tego joba\n",
    "\n",
    "Odpowiedzi:\n",
    "- czy zwiększenie granularności jest sensowne?\n",
    "- czy jest jakiś widoczny próg liczby sampli przy zwiększonej granularności?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
