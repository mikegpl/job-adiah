{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to build sklearn-like pipeline for transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd         \n",
    "import os.path\n",
    "\n",
    "N_JOBS = 6\n",
    "DEBUG=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path=\"../../data/csv/all_v2.csv\"):\n",
    "    dataframe = pd.read_csv(path, index_col=0)\n",
    "    return dataframe.loc[~dataframe[\"execTimeMs\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe(dataframe):\n",
    "    output = dataframe.dropna(axis=\"columns\")\n",
    "    targets = output[\"execTimeMs\"]\n",
    "    dropped = output[[\"command\", \"execTimeMs\", \"jobId\", \"ctime_mean\", \"ctime_max\", \"ctime_sum\", \"read_sum\",\"write_sum\",\"readSyscalls_sum\",\"writeSyscalls_sum\",\"readReal_sum\",\"writeReal_sum\",\"writeCancelled_sum\",\"rxBytes_sum\",\"rxPackets_sum\",\"rxErrors_sum\",\"rxDrop_sum\",\"rxFifo_sum\",\"rxFrame_sum\",\"rxCompressed_sum\",\"rxMulticast_sum\",\"txBytes_sum\",\"txPackets_sum\",\"txErrors_sum\",\"txDrop_sum\",\"txFifo_sum\",\"txColls_sum\",\"txCarrier_sum\",\"txCompressed_sum\",\"cpu_mean\",\"cpu_max\",\"memory_mean\",\"memory_max\"]]\n",
    "    features = output.drop(dropped.columns, axis=1)\n",
    "    return features, targets, dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, targets, dropped = prepare_dataframe(load_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "workflowName              object\n",
       "size                     float64\n",
       "executable                object\n",
       "args                      object\n",
       "inputs                    object\n",
       "outputs                   object\n",
       "name                      object\n",
       "cpu.manufacturer          object\n",
       "cpu.brand                 object\n",
       "cpu.speed                float64\n",
       "cpu.cores                  int64\n",
       "cpu.physicalCores          int64\n",
       "cpu.processors             int64\n",
       "mem.total                  int64\n",
       "mem.free                   int64\n",
       "mem.used                   int64\n",
       "mem.active                 int64\n",
       "mem.available              int64\n",
       "mem.buffers                int64\n",
       "mem.cached                 int64\n",
       "mem.slab                   int64\n",
       "mem.buffcache              int64\n",
       "mem.swaptotal              int64\n",
       "mem.swapused               int64\n",
       "mem.swapfree               int64\n",
       "total_cpus               float64\n",
       "avg_cpus                 float64\n",
       "avg_pods                 float64\n",
       "total_ram_available        int64\n",
       "average_ram_available    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_selector as selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_list(series):\n",
    "    def vectorize(list_string):\n",
    "        return len(eval(list_string))\n",
    "    return np.vectorize(vectorize)(series)\n",
    "\n",
    "def ListTransformer():\n",
    "    return FunctionTransformer(func=vectorize_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_transformer = Pipeline(steps=[(\"list\", ListTransformer()), (\"scaler\", StandardScaler())])\n",
    "list_features = list(['args', 'inputs', 'outputs'])\n",
    "\n",
    "numerical_transformer = StandardScaler()\n",
    "numerical_features = list(features.select_dtypes(include=\"number\").columns)\n",
    "\n",
    "categorical_transformer = OneHotEncoder(sparse=False, handle_unknown = \"ignore\")\n",
    "categorical_features = list(set(features.select_dtypes(include=\"object\").columns) ^ set(list_features))\n",
    "\n",
    "def make_classifying_preprocessor(additional_features=[\"read_sum\", \"write_sum\", \"cpu_mean\", \"memory_max\"]):\n",
    "    external_features = categorical_features + additional_features\n",
    "    return ColumnTransformer(\n",
    "            transformers=[('lists', list_transformer, list_features), \n",
    "                          ('num', numerical_transformer, numerical_features),\n",
    "                          ('cat', categorical_transformer, external_features)])\n",
    "\n",
    "def make_regression_preprocessor(additional_features=[\"read_sum\", \"write_sum\", \"cpu_mean\", \"memory_max\"]):\n",
    "    external_features = numerical_features + additional_features\n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('lists', list_transformer, list_features),            \n",
    "            ('num', numerical_transformer, external_features),  \n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "\n",
    "preprocessor = make_classifying_preprocessor(additional_features=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import percentileofscore\n",
    "import math\n",
    "\n",
    "def calculate_quantile_rank(labels, label):\n",
    "    return percentileofscore(labels, label) / 100\n",
    "\n",
    "def calculate_utilization_class(labels, label):\n",
    "    def label_for_rank(rank):\n",
    "        if rank > 0.75:\n",
    "            return 'very high'\n",
    "        elif rank > 0.5:\n",
    "            return 'high'\n",
    "        elif rank > 0.25:\n",
    "            return 'medium'\n",
    "        else:\n",
    "            return 'low'\n",
    "    return label_for_rank(calculate_quantile_rank(labels, label))\n",
    "\n",
    "def calculate_utilization_bucket(labels, label, num_buckets):\n",
    "    bucket_size = 1.0 / num_buckets\n",
    "    def bucket_for_rank(rank):\n",
    "        return str(math.floor(rank / bucket_size))\n",
    "    return bucket_for_rank(calculate_quantile_rank(labels, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline composition (with PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.linear_model import Lasso, SGDRegressor, ElasticNet, LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, HalvingGridSearchCV\n",
    "from sklearn.kernel_ridge import KernelRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_steps = [('pca', PCA(random_state=42))]\n",
    "dummy_pipeline = Pipeline(steps=base_steps +[('dummy', DummyRegressor())])\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_param_grid = {\n",
    "    'pca__n_components': np.arange(1, 50, 1),    \n",
    "}\n",
    "knn_param_grid = {\n",
    "    'knn__n_neighbors': np.arange(1, 30, 3),\n",
    "}\n",
    "regressor = ('knn', KNeighborsRegressor())\n",
    "full_pipeline = Pipeline(steps= base_steps + [regressor])\n",
    "grid_search = HalvingGridSearchCV(full_pipeline, {**knn_param_grid, **pca_param_grid}, cv=2, verbose=2, scoring=\"r2\", n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rae(actual, predicted):\n",
    "    \"\"\" Relative Absolute Error (aka Approximation Error) \"\"\"\n",
    "    EPSILON=1e-10\n",
    "    return np.sum(np.abs(actual - predicted)) / (np.sum(np.abs(actual - np.mean(actual))) + EPSILON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_absolute_percentage_error, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "rae_scorer=make_scorer(rae, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_regression_score(true, pred, scores=[r2_score, mean_absolute_error, mean_absolute_percentage_error, rae]):\n",
    "    executor = get_reusable_executor(max_workers=4)\n",
    "    results = executor.map(lambda fun: fun(true, pred), scores)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loky import get_reusable_executor\n",
    "\n",
    "def rate_regressor(X_train, y_train, X_test, y_test, regressor, regressor_params, verbose=10, aggressive_elimination=True, steps=base_steps, scoring=\"r2\"):\n",
    "    if DEBUG:\n",
    "        print(f\"Rating {regressor}\")\n",
    "    full_pipeline = Pipeline(steps= base_steps + [regressor])\n",
    "    vector_length = min(X_train.shape[0], X_train.shape[1])\n",
    "    pca_param_grid = {'pca__n_components': np.arange(1, vector_length, 1),}\n",
    "    grid_search = HalvingGridSearchCV(full_pipeline, {**pca_param_grid, **regressor_params}, cv=2, verbose=verbose, scoring=scoring, n_jobs=N_JOBS)\n",
    "    if DEBUG:\n",
    "        print(\"Evaluating grid search\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # scores\n",
    "    if DEBUG:\n",
    "        print(\"Predicting on test set\")\n",
    "    prediction = grid_search.best_estimator_.predict(X_test)\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(\"Calculating scores\")\n",
    "    r2, mae, mape, rae = calculate_regression_score(y_test, prediction)\n",
    "    if DEBUG:\n",
    "        print(\"Calculated scores on test set\")\n",
    "    adjusted_r2 = 1 - (1-r2)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)\n",
    "    return {\"r2\": r2, \"adjusted_r2\": adjusted_r2, \"mae\": mae, \"mape\": mape, \"rae\": rae,\"best_score\": grid_search.best_score_, \"params\": grid_search.best_params_}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here go regressor params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = (\"knn\", KNeighborsRegressor())\n",
    "knn_params = {'knn__n_neighbors': np.arange(1, 30, 1)}\n",
    "\n",
    "lasso = (\"lasso\", Lasso(random_state=5))\n",
    "lasso_params = {\"lasso__alpha\": np.arange(0.01, 1, 0.05)}\n",
    "\n",
    "mlp_regressor = (\"mlp\", MLPRegressor(random_state=5))\n",
    "mlp_params = {\"mlp__activation\": [\"relu\", \"logistic\"], \"mlp__hidden_layer_sizes\": [(100,), (100, 50,)]}\n",
    "\n",
    "dtr = (\"dtr\", DecisionTreeRegressor(random_state=5))\n",
    "dtr_params = {\"dtr__criterion\": [\"mse\", \"friedman_mse\", \"mae\", \"poisson\"], \"dtr__max_depth\": [5, 10, 15, 25]}\n",
    "\n",
    "en = (\"elasticnet\", ElasticNet(random_state=5))\n",
    "en_params = {\"elasticnet__alpha\": np.arange(0.01, 1, 0.05), \"elasticnet__l1_ratio\": np.arange(0, 1, 0.1)}\n",
    "\n",
    "svr = (\"svr\", SGDRegressor())\n",
    "svr_params = {\"svr__loss\": [\"squared_loss\", \"huber\", \"epsilon_insensitive\"], \"svr__penalty\": ['l2', 'l1', 'elasticnet'],\n",
    "             \"svr__alpha\": np.arange(0.0001, 0.2, 0.01), \"svr__max_iter\": [10000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_data(features, targets, regressors, verbose=10, pipeline_steps=base_steps):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.3, random_state=0)\n",
    "    df = pd.DataFrame(columns=[\"name\", \"pca\", \"adjusted_r2\",\"r2\", \"mae\", \"mape\", \"rae\", \"best_score\", \"params\"])\n",
    "    for (regressor, params) in regressors:\n",
    "        result = rate_regressor(X_train, y_train, X_test, y_test, regressor, params, verbose, pipeline_steps)\n",
    "        df = df.append({\"name\": regressor[0], **result, \"pca\": result[\"params\"][\"pca__n_components\"]}, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_data_explicit(X_train, X_test, y_train, y_test, regressors, verbose=10, pipeline_steps=base_steps, scoring=\"r2\"):\n",
    "    df = pd.DataFrame(columns=[\"name\", \"pca\", \"adjusted_r2\",\"r2\", \"mae\", \"mape\", \"best_score\", \"params\"])\n",
    "    for (regressor, params) in regressors:\n",
    "        result = rate_regressor(X_train, y_train, X_test, y_test, regressor, params, verbose, pipeline_steps, scoring=scoring)\n",
    "        df = df.append({\"name\": regressor[0], **result, \"pca\": result[\"params\"][\"pca__n_components\"]}, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_classifiers_for_data(features, targets, classifiers, verbose=10, pipeline_steps=base_steps):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.3, random_state=0)\n",
    "    df = pd.DataFrame(columns=[\"name\", \"pca\", \"accuracy\",\"balanced_accuracy\", \"f1_micro\", \"f1_macro\", \"params\"])\n",
    "    for (classifier, params) in classifiers:\n",
    "        result = rate_classifier(X_train, y_train, X_test, y_test, classifier, params, verbose, pipeline_steps)\n",
    "        df = df.append({\"name\": classifier[0], **result, \"pca\": result[\"params\"][\"pca__n_components\"]}, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_dataset(dataframe, regressors, verbose=2):\n",
    "    print(f\"Rating dataset of len {len(dataframe)}\")\n",
    "    features, targets, _ = prepare_dataframe(dataframe[:10000])\n",
    "    features = preprocessor.fit_transform(features)\n",
    "    rate_data(features, targets, regressors, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_regressors = [\n",
    "    (knn, knn_params),\n",
    "    (dtr, dtr_params),\n",
    "    (lasso, lasso_params),\n",
    "    (mlp_regressor, mlp_params),\n",
    "    (en, en_params),\n",
    "    (svr, svr_params),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_experiment():\n",
    "    print(\"Rating jobs datasets\")\n",
    "    for dataset in dfs_for_jobs:\n",
    "        print(dataset.iloc[0][\"name\"])\n",
    "        rate_dataset(dataset, basic_regressors)\n",
    "\n",
    "    print(\"Rating common datasets\")\n",
    "    for dataset in datasets:\n",
    "        rate_dataset(dataset, basic_regressors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rate_dataset(dfs_for_jobs[8], verbose=0)\n",
    "# rate_dataset(dfs_for_jobs[1], basic_regressors, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eksperyment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zmierzyć skuteczności najlepszych pipelinów dla każdego joba, zobaczyć czy warto schodzić w dół pod wzgledem błędów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = load_data().dropna(axis=\"columns\")\n",
    "raw_datasets = { x:pd.DataFrame(y) for x, y in full_df.groupby('name', as_index=False)}\n",
    "datasets_split = {x:train_test_split(df, random_state=0, train_size=0.75) for x,df in raw_datasets.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numerical_pipeline_data_big(data, resources=[\"read_sum\", \"write_sum\", \"cpu_mean\", \"memory_max\"]):\n",
    "    \"\"\"\n",
    "    Raw data enhanced with resource utilization quantile scores, but scores are assigned - not predicted\n",
    "    \"\"\"\n",
    "    features, labels, dropped = prepare_dataframe(data)\n",
    "    for resource in resources:\n",
    "        features[resource] = dropped[resource].map(lambda value: calculate_quantile_rank(dropped[resource], value))\n",
    "    features = make_regression_preprocessor(resources).fit_transform(features)\n",
    "    return pd.DataFrame(features, index=labels.index), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_symbolic_regression_data(data, resources=[\"read_sum\", \"write_sum\", \"cpu_mean\", \"memory_max\"]):\n",
    "    features, labels, dropped = prepare_dataframe(data)\n",
    "    for resource in resources:\n",
    "        features[resource] = dropped[resource].map(lambda value: calculate_quantile_rank(dropped[resource], value))\n",
    "    for list_feature in list_features:\n",
    "        features[list_feature] = features[list_feature].map(lambda list_val: vectorize_list(list_val))\n",
    "    return pd.DataFrame(features, index=labels.index), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categorical_pipeline_data_big(data, resources=[\"read_sum\", \"write_sum\", \"cpu_mean\", \"memory_max\"]):\n",
    "    features, labels, dropped = prepare_dataframe(data)\n",
    "    for resource in resources:\n",
    "        features[resource] = dropped[resource].map(lambda value: calculate_utilization_bucket(dropped[resource], value, num_buckets=8))\n",
    "    features = make_classifying_preprocessor(resources).fit_transform(features)\n",
    "    return pd.DataFrame(features, index=labels.index), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobId</th>\n",
       "      <th>read_sum</th>\n",
       "      <th>write_sum</th>\n",
       "      <th>readSyscalls_sum</th>\n",
       "      <th>writeSyscalls_sum</th>\n",
       "      <th>readReal_sum</th>\n",
       "      <th>writeReal_sum</th>\n",
       "      <th>writeCancelled_sum</th>\n",
       "      <th>rxBytes_sum</th>\n",
       "      <th>rxPackets_sum</th>\n",
       "      <th>...</th>\n",
       "      <th>mem.slab</th>\n",
       "      <th>mem.buffcache</th>\n",
       "      <th>mem.swaptotal</th>\n",
       "      <th>mem.swapused</th>\n",
       "      <th>mem.swapfree</th>\n",
       "      <th>total_cpus</th>\n",
       "      <th>avg_cpus</th>\n",
       "      <th>avg_pods</th>\n",
       "      <th>total_ram_available</th>\n",
       "      <th>average_ram_available</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>74622</th>\n",
       "      <td>o5B3JL1c0-1-60</td>\n",
       "      <td>393064.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32768.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>509407232</td>\n",
       "      <td>4113100800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.69</td>\n",
       "      <td>3.422500</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>47131832000</td>\n",
       "      <td>1.178296e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39958</th>\n",
       "      <td>VdebhcHGz-1-86</td>\n",
       "      <td>423392.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>317.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32768.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13186.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>...</td>\n",
       "      <td>836173824</td>\n",
       "      <td>4935049216</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19.60</td>\n",
       "      <td>3.920000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>68207180000</td>\n",
       "      <td>1.364144e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18723</th>\n",
       "      <td>6sXqyD-cD-1-372</td>\n",
       "      <td>382827.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32768.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>705224704</td>\n",
       "      <td>10695143424</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19.60</td>\n",
       "      <td>3.920000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>68207180000</td>\n",
       "      <td>1.364144e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81198</th>\n",
       "      <td>uwu6POi40-1-86</td>\n",
       "      <td>436311.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>335.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32768.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>317255680</td>\n",
       "      <td>2782396416</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.52</td>\n",
       "      <td>3.920000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>81848584000</td>\n",
       "      <td>1.364143e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6678</th>\n",
       "      <td>1XRbCZX-m-1-34</td>\n",
       "      <td>429309.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32768.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>317988864</td>\n",
       "      <td>5578797056</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15.68</td>\n",
       "      <td>3.920000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>54565728000</td>\n",
       "      <td>1.364143e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28118</th>\n",
       "      <td>yXB4dcCCr-1-112</td>\n",
       "      <td>480781.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>437.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>49152.0</td>\n",
       "      <td>32768.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11340.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>...</td>\n",
       "      <td>206446592</td>\n",
       "      <td>3237965824</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.72</td>\n",
       "      <td>1.930000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>25340644000</td>\n",
       "      <td>6.335161e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14761</th>\n",
       "      <td>Qt1V_tSyk-1-138</td>\n",
       "      <td>244818.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32768.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11586.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1794420736</td>\n",
       "      <td>8417120256</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17.75</td>\n",
       "      <td>5.916667</td>\n",
       "      <td>44.333333</td>\n",
       "      <td>66543572000</td>\n",
       "      <td>2.218119e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53990</th>\n",
       "      <td>ddDQIVS8y-1-502</td>\n",
       "      <td>461037.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32768.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>329924608</td>\n",
       "      <td>3229777920</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.52</td>\n",
       "      <td>3.920000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>81848616000</td>\n",
       "      <td>1.364144e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74749</th>\n",
       "      <td>oXReaKxEP-1-138</td>\n",
       "      <td>398153.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>298.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32768.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12266.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>602771456</td>\n",
       "      <td>8635564032</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19.60</td>\n",
       "      <td>3.920000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>68207180000</td>\n",
       "      <td>1.364144e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37771</th>\n",
       "      <td>P2WYkNQ_Y-1-190</td>\n",
       "      <td>430776.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>326.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32768.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13778.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>332304384</td>\n",
       "      <td>2898698240</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.69</td>\n",
       "      <td>3.422500</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>47131800000</td>\n",
       "      <td>1.178295e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 jobId  read_sum  write_sum  readSyscalls_sum  \\\n",
       "74622   o5B3JL1c0-1-60  393064.0        9.0             289.0   \n",
       "39958   VdebhcHGz-1-86  423392.0        9.0             317.0   \n",
       "18723  6sXqyD-cD-1-372  382827.0        9.0             280.0   \n",
       "81198   uwu6POi40-1-86  436311.0        9.0             335.0   \n",
       "6678    1XRbCZX-m-1-34  429309.0        9.0             326.0   \n",
       "...                ...       ...        ...               ...   \n",
       "28118  yXB4dcCCr-1-112  480781.0        9.0             437.0   \n",
       "14761  Qt1V_tSyk-1-138  244818.0        9.0             145.0   \n",
       "53990  ddDQIVS8y-1-502  461037.0       19.0             377.0   \n",
       "74749  oXReaKxEP-1-138  398153.0        9.0             298.0   \n",
       "37771  P2WYkNQ_Y-1-190  430776.0        9.0             326.0   \n",
       "\n",
       "       writeSyscalls_sum  readReal_sum  writeReal_sum  writeCancelled_sum  \\\n",
       "74622                9.0           0.0        32768.0                 0.0   \n",
       "39958                9.0           0.0        32768.0                 0.0   \n",
       "18723                9.0           0.0        32768.0                 0.0   \n",
       "81198                9.0           0.0        32768.0                 0.0   \n",
       "6678                 9.0           0.0        32768.0                 0.0   \n",
       "...                  ...           ...            ...                 ...   \n",
       "28118                9.0       49152.0        32768.0                 0.0   \n",
       "14761                9.0           0.0        32768.0                 0.0   \n",
       "53990               10.0           0.0        32768.0                 0.0   \n",
       "74749                9.0           0.0        32768.0                 0.0   \n",
       "37771                9.0           0.0        32768.0                 0.0   \n",
       "\n",
       "       rxBytes_sum  rxPackets_sum  ...    mem.slab  mem.buffcache  \\\n",
       "74622          0.0            0.0  ...   509407232     4113100800   \n",
       "39958      13186.0           42.0  ...   836173824     4935049216   \n",
       "18723          0.0            0.0  ...   705224704    10695143424   \n",
       "81198          0.0            0.0  ...   317255680     2782396416   \n",
       "6678           0.0            0.0  ...   317988864     5578797056   \n",
       "...            ...            ...  ...         ...            ...   \n",
       "28118      11340.0           27.0  ...   206446592     3237965824   \n",
       "14761      11586.0           28.0  ...  1794420736     8417120256   \n",
       "53990          0.0            0.0  ...   329924608     3229777920   \n",
       "74749      12266.0           32.0  ...   602771456     8635564032   \n",
       "37771      13778.0           50.0  ...   332304384     2898698240   \n",
       "\n",
       "       mem.swaptotal  mem.swapused  mem.swapfree  total_cpus  avg_cpus  \\\n",
       "74622              0             0             0       13.69  3.422500   \n",
       "39958              0             0             0       19.60  3.920000   \n",
       "18723              0             0             0       19.60  3.920000   \n",
       "81198              0             0             0       23.52  3.920000   \n",
       "6678               0             0             0       15.68  3.920000   \n",
       "...              ...           ...           ...         ...       ...   \n",
       "28118              0             0             0        7.72  1.930000   \n",
       "14761              0             0             0       17.75  5.916667   \n",
       "53990              0             0             0       23.52  3.920000   \n",
       "74749              0             0             0       19.60  3.920000   \n",
       "37771              0             0             0       13.69  3.422500   \n",
       "\n",
       "         avg_pods  total_ram_available  average_ram_available  \n",
       "74622  110.000000          47131832000           1.178296e+10  \n",
       "39958  110.000000          68207180000           1.364144e+10  \n",
       "18723  110.000000          68207180000           1.364144e+10  \n",
       "81198  110.000000          81848584000           1.364143e+10  \n",
       "6678   110.000000          54565728000           1.364143e+10  \n",
       "...           ...                  ...                    ...  \n",
       "28118   30.500000          25340644000           6.335161e+09  \n",
       "14761   44.333333          66543572000           2.218119e+10  \n",
       "53990  110.000000          81848616000           1.364144e+10  \n",
       "74749  110.000000          68207180000           1.364144e+10  \n",
       "37771  110.000000          47131800000           1.178295e+10  \n",
       "\n",
       "[107 rows x 63 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_split['add_replace'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przebieg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trenujemy pipeline ogólny, liczymy jego skuteczności dla każdego typu jobów. Z eksperymentu 1. - najlepszy pipeline to był:\n",
    "\n",
    "dtr, mae, pca 71\n",
    "\n",
    "dla każdego typu jobów trenujemy dla niego pipeline, liczymy skuteczności"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = datasets_split['add_replace']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment4():\n",
    "    exp4_resources = [\"read_sum\", \"write_sum\", \"cpu_max\", \"cpu_mean\", \"memory_mean\", \"memory_max\"]\n",
    "        \n",
    "    big_regressor = Pipeline([ # test pipeline\n",
    "        ('pca', PCA(random_state=42, n_components=71)),\n",
    "#         ('knn', KNeighborsRegressor(n_neighbors=11))\n",
    "        ('dtr', DecisionTreeRegressor(criterion=\"mae\"))\n",
    "    ])\n",
    "    print(\"Preparing data for big regressor\")\n",
    "    X, y = get_numerical_pipeline_data_big(full_df, exp4_resources)\n",
    "    train_indices = np.concatenate([train.index for (_, (train, _)) in datasets_split.items()])\n",
    "    test_indices = np.concatenate([test.index for (_, (_, test)) in datasets_split.items()])\n",
    "    print(f\"{len(X)} {len(y)} {len(train_indices)} {len(test_indices)}\")\n",
    "    X_train, X_test, y_train, y_test = X.loc[train_indices], X.loc[test_indices], y.loc[train_indices], y.loc[test_indices]\n",
    "    print(f\"Making split with test as {len(test_indices)/(len(test_indices) + len(train_indices))} of dataset\")\n",
    "    \n",
    "    print(\"Training big regressor with train data\")\n",
    "    big_regressor.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Predicting with big regressor on test data\")\n",
    "    y_predicted = big_regressor.predict(X_test)\n",
    "    \n",
    "    print(\"Rating big regressor's overall performance\")\n",
    "    [r2, mae, mape, rae_score] = calculate_regression_score(y_test, y_predicted, [r2_score, mean_absolute_error, mean_absolute_percentage_error, rae])\n",
    "    print(f\"Scores for big regressor:\")\n",
    "    print(f\"R2: {r2}\")\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"MAPE: {mape}\")\n",
    "    print(f\"RAE: {rae_score}\")\n",
    "    print()\n",
    "    dataframes = []\n",
    "    for (job, (train, test)) in datasets_split.items():\n",
    "        print(f\"Comparing big regressor vs local regressor for job {job}\")\n",
    "        print(\"Preparing data for local regressor\")\n",
    "        joint_df = pd.concat([train, test])\n",
    "        local_X, local_y = get_numerical_pipeline_data_big(joint_df, exp4_resources)\n",
    "        print(f\"{len(local_X)} {len(train.index)} {len(test.index)}\")\n",
    "        local_X_train, local_X_test, local_y_train, local_y_test = local_X.loc[train.index], local_X.loc[test.index], local_y.loc[train.index], local_y.loc[test.index]\n",
    "        \n",
    "        print(f\"Rating local regressors for job {job}\")\n",
    "        regressor_df = rate_data_explicit(local_X_train, local_X_test, local_y_train, local_y_test, basic_regressors, verbose=0)\n",
    "        print(regressor_df.head())\n",
    "        print(\"Preparing data for big regressor\")\n",
    "        local_X_test, local_y_test = X.loc[test.index], y.loc[test.index]\n",
    "        \n",
    "        print(f\"Rating big regressor for job {job}\")\n",
    "        local_predicted = big_regressor.predict(local_X_test)\n",
    "        [r2, mae, mape, rae_score] = calculate_regression_score(local_predicted, local_y_test, [r2_score, mean_absolute_error, mean_absolute_percentage_error, rae])\n",
    "        regressor_df = regressor_df.append({\"name\": \"big\", **{\"r2\": r2, \"mae\": mae, \"mape\": mape, \"rae\": rae_score}, \"pca\": \"xD\"}, ignore_index=True)\n",
    "        regressor_df[\"job\"] = job\n",
    "        regressor_df[\"size\"] = len(joint_df)\n",
    "        dataframes.append(regressor_df)\n",
    "        \n",
    "    return pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment4_numerical_rae():\n",
    "    exp4_resources = [\"read_sum\", \"write_sum\", \"cpu_max\", \"cpu_mean\", \"memory_mean\", \"memory_max\"]\n",
    "        \n",
    "    \n",
    "    train_indices = np.concatenate([train.index for (_, (train, _)) in datasets_split.items()])\n",
    "    test_indices = np.concatenate([test.index for (_, (_, test)) in datasets_split.items()])\n",
    "    dataframes = []\n",
    "    for (job, (train, test)) in datasets_split.items():\n",
    "        print(\"Preparing data for local regressor\")\n",
    "        joint_df = pd.concat([train, test])\n",
    "        local_X, local_y = get_numerical_pipeline_data_big(joint_df, exp4_resources)\n",
    "        print(f\"{len(local_X)} {len(train.index)} {len(test.index)}\")\n",
    "        local_X_train, local_X_test, local_y_train, local_y_test = local_X.loc[train.index], local_X.loc[test.index], local_y.loc[train.index], local_y.loc[test.index]\n",
    "        \n",
    "        print(f\"Rating local regressors for job {job}\")\n",
    "        regressor_df = rate_data_explicit(local_X_train, local_X_test, local_y_train, local_y_test, basic_regressors, verbose=0, scoring=rae_scorer)\n",
    "        print(regressor_df.head())\n",
    "        regressor_df[\"job\"] = job\n",
    "        regressor_df[\"size\"] = len(joint_df)\n",
    "        dataframes.append(regressor_df)\n",
    "        \n",
    "    return pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_or_run(file, runner):\n",
    "    if not os.path.isfile(file):\n",
    "        print(f\"Running experiment {file}\")\n",
    "        dataframe = runner()\n",
    "        dataframe.to_csv(file)\n",
    "    else:\n",
    "        dataframe = pd.read_csv(file).round(2)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_JOBS=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp4_r2_df_incomplete = load_or_run(\"data/exp4_optimize_r2.csv\", run_experiment4)\n",
    "exp4_rae_incomplete = load_or_run(\"data/exp4_optimize_rae.csv\", run_experiment4_numerical_rae)\n",
    "# exp4_r2_df_memory_intensive = load_or_run(\"data/exp4_optimize_r2_memint.csv\", run_experiment4_memory_intensive)\n",
    "# exp4_r2_categorical_incomplete = load_or_run(\"data/exp4_optimize_r2_categorical.csv\", run_experiment4_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_for_symbolic_regression():\n",
    "    exp4_resources = [\"read_sum\", \"write_sum\", \"cpu_max\", \"cpu_mean\", \"memory_mean\", \"memory_max\"]\n",
    "            \n",
    "    train_indices = np.concatenate([train.index for (_, (train, _)) in datasets_split.items()])\n",
    "    test_indices = np.concatenate([test.index for (_, (_, test)) in datasets_split.items()])\n",
    "    for (job, (train, test)) in datasets_split.items():\n",
    "        print(f\"Preparing data for symbolic regressor: {job}, {len(train) + len(test)}\")\n",
    "        joint_df = pd.concat([train, test])\n",
    "        local_X, local_y = get_symbolic_regression_data(joint_df, exp4_resources)\n",
    "        local_X_train, local_X_test, local_y_train, local_y_test = local_X.loc[train.index], local_X.loc[test.index], local_y.loc[train.index], local_y.loc[test.index]\n",
    "        \n",
    "        joint_train = local_X_train.join(local_y_train)\n",
    "        joint_test = local_X_test.join(local_y_test)\n",
    "\n",
    "        joint_train.to_csv(f\"data/symbolic/{job}_train.csv\")\n",
    "        joint_test.to_csv(f\"data/symbolic/{job}_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_for_symbolic_regression_single_step():\n",
    "    exp4_resources = []\n",
    "            \n",
    "    train_indices = np.concatenate([train.index for (_, (train, _)) in datasets_split.items()])\n",
    "    test_indices = np.concatenate([test.index for (_, (_, test)) in datasets_split.items()])\n",
    "    for (job, (train, test)) in datasets_split.items():\n",
    "        print(f\"Preparing data for symbolic regressor: {job}, {len(train) + len(test)}\")\n",
    "        joint_df = pd.concat([train, test])\n",
    "        local_X, local_y = get_symbolic_regression_data(joint_df, exp4_resources)\n",
    "        local_X_train, local_X_test, local_y_train, local_y_test = local_X.loc[train.index], local_X.loc[test.index], local_y.loc[train.index], local_y.loc[test.index]\n",
    "        \n",
    "        joint_train = local_X_train.join(local_y_train)\n",
    "        joint_test = local_X_test.join(local_y_test)\n",
    "\n",
    "        joint_train.to_csv(f\"data/symbolicRaw/{job}_train.csv\")\n",
    "        joint_test.to_csv(f\"data/symbolicRaw/{job}_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_for_symbolic_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data for symbolic regressor: add_replace, 427\n",
      "427: 320 / 107\n",
      "Preparing data for symbolic regressor: alignment_to_reference, 427\n",
      "427: 320 / 107\n",
      "Preparing data for symbolic regressor: bwa-index, 63\n",
      "63: 47 / 16\n",
      "Preparing data for symbolic regressor: combine_variants, 62\n",
      "62: 46 / 16\n",
      "Preparing data for symbolic regressor: dedup, 427\n",
      "427: 320 / 107\n",
      "Preparing data for symbolic regressor: faidx, 63\n",
      "63: 47 / 16\n",
      "Preparing data for symbolic regressor: filtering_indel, 62\n",
      "62: 46 / 16\n",
      "Preparing data for symbolic regressor: filtering_snp, 62\n",
      "62: 46 / 16\n",
      "Preparing data for symbolic regressor: genotype_gvcfs, 1259\n",
      "1259: 944 / 315\n",
      "Preparing data for symbolic regressor: haplotype_caller, 8539\n",
      "8539: 6404 / 2135\n",
      "Preparing data for symbolic regressor: indel_realign, 427\n",
      "427: 320 / 107\n",
      "Preparing data for symbolic regressor: mAdd, 160\n",
      "160: 120 / 40\n",
      "Preparing data for symbolic regressor: mBackground, 15340\n",
      "15340: 11505 / 3835\n",
      "Preparing data for symbolic regressor: mBgModel, 160\n",
      "160: 120 / 40\n",
      "Preparing data for symbolic regressor: mConcatFit, 160\n",
      "160: 120 / 40\n",
      "Preparing data for symbolic regressor: mDiffFit, 65323\n",
      "65323: 48992 / 16331\n",
      "Preparing data for symbolic regressor: mImgtbl, 157\n",
      "157: 117 / 40\n",
      "Preparing data for symbolic regressor: mJPEG, 43\n",
      "43: 32 / 11\n",
      "Preparing data for symbolic regressor: mProject, 11316\n",
      "11316: 8487 / 2829\n",
      "Preparing data for symbolic regressor: mProjectPP, 5551\n",
      "5551: 4163 / 1388\n",
      "Preparing data for symbolic regressor: mShrink, 43\n",
      "43: 32 / 11\n",
      "Preparing data for symbolic regressor: mViewer, 156\n",
      "156: 117 / 39\n",
      "Preparing data for symbolic regressor: merge_gcvf, 61\n",
      "61: 45 / 16\n",
      "Preparing data for symbolic regressor: realign_target_creator, 427\n",
      "427: 320 / 107\n",
      "Preparing data for symbolic regressor: select_variants_indel, 62\n",
      "62: 46 / 16\n",
      "Preparing data for symbolic regressor: select_variants_snp, 62\n",
      "62: 46 / 16\n",
      "Preparing data for symbolic regressor: seq_dict, 63\n",
      "63: 47 / 16\n",
      "Preparing data for symbolic regressor: sort_sam, 427\n",
      "427: 320 / 107\n"
     ]
    }
   ],
   "source": [
    "export_for_symbolic_regression_single_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wyniki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykresy:\n",
    "- najlepsza skuteczność regresora (min mape/rae) vs liczba sampli\n",
    "- skuteczności 5 najlepszych regresorów dla każdego (grid chart)\n",
    "- najlepsza skuteczność regresora dla joba vs skuteczność dużego regresora dla tego joba\n",
    "\n",
    "Odpowiedzi:\n",
    "- czy zwiększenie granularności jest sensowne?\n",
    "- czy jest jakiś widoczny próg liczby sampli przy zwiększonej granularności?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
