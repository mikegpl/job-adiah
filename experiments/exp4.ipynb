{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to build sklearn-like pipeline for transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd         \n",
    "import os.path\n",
    "\n",
    "N_JOBS = 7\n",
    "DEBUG=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path=\"../../data/csv/all.csv\"):\n",
    "    dataframe = pd.read_csv(path, index_col=0)\n",
    "    return dataframe.loc[~dataframe[\"execTimeMs\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe(dataframe):\n",
    "    output = dataframe.dropna(axis=\"columns\")\n",
    "    targets = output[\"execTimeMs\"]\n",
    "    dropped = output[[\"command\", \"execTimeMs\", \"jobId\", \"ctime_mean\", \"ctime_max\", \"ctime_sum\", \"read_sum\",\"write_sum\",\"readSyscalls_sum\",\"writeSyscalls_sum\",\"readReal_sum\",\"writeReal_sum\",\"writeCancelled_sum\",\"rxBytes_sum\",\"rxPackets_sum\",\"rxErrors_sum\",\"rxDrop_sum\",\"rxFifo_sum\",\"rxFrame_sum\",\"rxCompressed_sum\",\"rxMulticast_sum\",\"txBytes_sum\",\"txPackets_sum\",\"txErrors_sum\",\"txDrop_sum\",\"txFifo_sum\",\"txColls_sum\",\"txCarrier_sum\",\"txCompressed_sum\",\"cpu_mean\",\"cpu_max\",\"memory_mean\",\"memory_max\"]]\n",
    "    features = output.drop(dropped.columns, axis=1)\n",
    "    return features, targets, dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, targets, dropped = prepare_dataframe(load_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "workflowName          object\n",
       "size                 float64\n",
       "executable            object\n",
       "args                  object\n",
       "inputs                object\n",
       "outputs               object\n",
       "name                  object\n",
       "cpu.manufacturer      object\n",
       "cpu.brand             object\n",
       "cpu.speed            float64\n",
       "cpu.cores              int64\n",
       "cpu.physicalCores      int64\n",
       "cpu.processors         int64\n",
       "mem.total              int64\n",
       "mem.free               int64\n",
       "mem.used               int64\n",
       "mem.active             int64\n",
       "mem.available          int64\n",
       "mem.buffers            int64\n",
       "mem.cached             int64\n",
       "mem.slab               int64\n",
       "mem.buffcache          int64\n",
       "mem.swaptotal          int64\n",
       "mem.swapused           int64\n",
       "mem.swapfree           int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_selector as selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_list(series):\n",
    "    def vectorize(list_string):\n",
    "        return len(eval(list_string))\n",
    "    return np.vectorize(vectorize)(series)\n",
    "\n",
    "def ListTransformer():\n",
    "    return FunctionTransformer(func=vectorize_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_transformer = Pipeline(steps=[(\"list\", ListTransformer()), (\"scaler\", StandardScaler())])\n",
    "list_features = list(['args', 'inputs', 'outputs'])\n",
    "\n",
    "numerical_transformer = StandardScaler()\n",
    "numerical_features = list(features.select_dtypes(include=\"number\").columns)\n",
    "\n",
    "categorical_transformer = OneHotEncoder(sparse=False, handle_unknown = \"ignore\")\n",
    "categorical_features = list(set(features.select_dtypes(include=\"object\").columns) ^ set(list_features))\n",
    "\n",
    "def make_classifying_preprocessor(additional_features=[\"read_sum\", \"write_sum\", \"cpu_mean\", \"memory_max\"]):\n",
    "    external_features = categorical_features + additional_features\n",
    "    return ColumnTransformer(\n",
    "            transformers=[('lists', list_transformer, list_features), \n",
    "                          ('num', numerical_transformer, numerical_features),\n",
    "                          ('cat', categorical_transformer, external_features)])\n",
    "\n",
    "def make_regression_preprocessor(additional_features=[\"read_sum\", \"write_sum\", \"cpu_mean\", \"memory_max\"]):\n",
    "    external_features = numerical_features + additional_features\n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('lists', list_transformer, list_features),            \n",
    "            ('num', numerical_transformer, external_features),  \n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "\n",
    "preprocessor = make_classifying_preprocessor(additional_features=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import percentileofscore\n",
    "import math\n",
    "\n",
    "def calculate_quantile_rank(labels, label):\n",
    "    return percentileofscore(labels, label) / 100\n",
    "\n",
    "def calculate_utilization_class(labels, label):\n",
    "    def label_for_rank(rank):\n",
    "        if rank > 0.75:\n",
    "            return 'very high'\n",
    "        elif rank > 0.5:\n",
    "            return 'high'\n",
    "        elif rank > 0.25:\n",
    "            return 'medium'\n",
    "        else:\n",
    "            return 'low'\n",
    "    return label_for_rank(calculate_quantile_rank(labels, label))\n",
    "\n",
    "def calculate_utilization_bucket(labels, label, num_buckets):\n",
    "    bucket_size = 1.0 / num_buckets\n",
    "    def bucket_for_rank(rank):\n",
    "        return str(math.floor(rank / bucket_size))\n",
    "    return bucket_for_rank(calculate_quantile_rank(labels, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline composition (with PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.linear_model import Lasso, SGDRegressor, ElasticNet, LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, HalvingGridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_steps = [('pca', PCA(random_state=42))]\n",
    "dummy_pipeline = Pipeline(steps=base_steps +[('dummy', DummyRegressor())])\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_param_grid = {\n",
    "    'pca__n_components': np.arange(1, 50, 3),    \n",
    "}\n",
    "knn_param_grid = {\n",
    "    'knn__n_neighbors': np.arange(1, 30, 3),\n",
    "}\n",
    "regressor = ('knn', KNeighborsRegressor())\n",
    "full_pipeline = Pipeline(steps= base_steps + [regressor])\n",
    "grid_search = HalvingGridSearchCV(full_pipeline, {**knn_param_grid, **pca_param_grid}, cv=2, verbose=2, scoring=\"r2\", n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rae(actual, predicted):\n",
    "    \"\"\" Relative Absolute Error (aka Approximation Error) \"\"\"\n",
    "    EPSILON=1e-10\n",
    "    return np.sum(np.abs(actual - predicted)) / (np.sum(np.abs(actual - np.mean(actual))) + EPSILON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_regression_score(true, pred, scores=[r2_score, mean_absolute_error, mean_absolute_percentage_error, rae]):\n",
    "    executor = get_reusable_executor(max_workers=4)\n",
    "    results = executor.map(lambda fun: fun(true, pred), scores)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loky import get_reusable_executor\n",
    "\n",
    "def rate_regressor(X_train, y_train, X_test, y_test, regressor, regressor_params, verbose=10, aggressive_elimination=True, steps=base_steps):\n",
    "    if DEBUG:\n",
    "        print(f\"Rating {regressor}\")\n",
    "    full_pipeline = Pipeline(steps= base_steps + [regressor])\n",
    "    vector_length = X_train.shape[1]\n",
    "    pca_param_grid = {'pca__n_components': np.arange(1, vector_length, 1),}\n",
    "    grid_search = HalvingGridSearchCV(full_pipeline, {**pca_param_grid, **regressor_params}, cv=2, verbose=verbose, scoring=\"r2\", n_jobs=N_JOBS)\n",
    "    if DEBUG:\n",
    "        print(\"Evaluating grid search\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # scores\n",
    "    if DEBUG:\n",
    "        print(\"Predicting on test set\")\n",
    "    prediction = grid_search.best_estimator_.predict(X_test)\n",
    "    \n",
    "    if DEBUG:\n",
    "        print(\"Calculating scores\")\n",
    "    r2, mae, mape, rae = calculate_regression_score(y_test, prediction)\n",
    "    if DEBUG:\n",
    "        print(\"Calculated scores on test set\")\n",
    "    adjusted_r2 = 1 - (1-r2)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)\n",
    "    return {\"r2\": r2, \"adjusted_r2\": adjusted_r2, \"mae\": mae, \"mape\": mape, \"rae\": rae,\"best_score\": grid_search.best_score_, \"params\": grid_search.best_params_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, balanced_accuracy_score, roc_auc_score\n",
    "\n",
    "def rate_classifier(X_train, y_train, X_test, y_test, classifier, classifier_params, verbose=10, aggressive_elimination=True, steps=base_steps):\n",
    "    print(f\"Rating {classifier}\")\n",
    "    full_pipeline = Pipeline(steps= base_steps + [classifier])\n",
    "    vector_length = X_train.shape[1]\n",
    "    pca_param_grid = {'pca__n_components': np.arange(1, vector_length, 1),}\n",
    "    grid_search = HalvingGridSearchCV(full_pipeline, {**pca_param_grid, **classifier_params}, cv=2, verbose=verbose, scoring=\"accuracy\", n_jobs=N_JOBS)\n",
    "    print(\"Evaluating grid search\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # scores\n",
    "    print(\"Predicting on test set\")\n",
    "    prediction = grid_search.best_estimator_.predict(X_test)\n",
    "    \n",
    "    print(\"Calculating scores\")\n",
    "    executor = get_reusable_executor(max_workers=5, timeout=5)\n",
    "    \n",
    "    scores = [\n",
    "        lambda true, pred: f1_score(true, pred, average=\"micro\"),\n",
    "        lambda true, pred: f1_score(true, pred, average=\"macro\"),\n",
    "        lambda true, pred: accuracy_score(true, pred), \n",
    "        lambda true, pred: balanced_accuracy_score(true, pred)\n",
    "    ]\n",
    "    results = executor.map(lambda fun: fun(y_test, prediction), scores)\n",
    "    print(\"Calculated scores on test set\")\n",
    "    micro, macro, accuracy, balanced_accuracy = list(results)\n",
    "    return {\"f1_micro\": micro, \"f1_macro\": macro, \"accuracy\": accuracy, \"balanced_accuracy\": balanced_accuracy, \"params\": grid_search.best_params_}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here go regressor params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = (\"knn\", KNeighborsRegressor())\n",
    "knn_params = {'knn__n_neighbors': np.arange(1, 30, 1)}\n",
    "\n",
    "dtr = (\"dtr\", DecisionTreeRegressor(random_state=5))\n",
    "dtr_params = {\"dtr__criterion\": [\"mse\", \"friedman_mse\", \"mae\", \"poisson\"]}\n",
    "\n",
    "lasso = (\"lasso\", Lasso(random_state=5))\n",
    "lasso_params = {\"lasso__alpha\": np.arange(0.01, 1, 0.05)}\n",
    "\n",
    "en = (\"elasticnet\", ElasticNet(random_state=5))\n",
    "en_params = {\"elasticnet__alpha\": np.arange(0.01, 1, 0.05), \"elasticnet__l1_ratio\": np.arange(0, 1, 0.1)}\n",
    "\n",
    "svr = (\"svr\", SGDRegressor())\n",
    "svr_params = {\"svr__loss\": [\"squared_loss\", \"huber\", \"epsilon_insensitive\"], \"svr__penalty\": ['l2', 'l1', 'elasticnet'],\n",
    "             \"svr__alpha\": np.arange(0.0001, 0.2, 0.01), \"svr__max_iter\": [10000]}\n",
    "\n",
    "rf = (\"rf\", RandomForestRegressor())\n",
    "rf_params = {\"rf__n_estimators\": np.arange(5, 100, 5), \"rf__criterion\": [\"mae\", \"mse\"], \"rf__max_features\": [\"auto\", \"sqrt\", \"log2\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here go classifier params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_classifier = (\"knn\", KNeighborsClassifier())\n",
    "knn_clf_params = {'knn__n_neighbors': np.arange(1, 30, 1)}\n",
    "\n",
    "dtr_classifier = (\"dtr\", DecisionTreeClassifier(random_state=5))\n",
    "dtr_clf_params = {\"dtr__criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "mlp_classifier = (\"mlp\", MLPClassifier())\n",
    "mlp_clf_params = {\"mlp__hidden_layer_sizes\": np.arange(1,200, 10),          \n",
    "                  \"mlp__activation\": [\"logistic\", \"tanh\", \"relu\"],         \n",
    "                  \"mlp__activation\": [\"logistic\"],         \n",
    "#                   \"mlp__alpha\": np.arange(0.01, 0.1, 0.01)\n",
    "                 }\n",
    "\n",
    "svc = (\"svc\", SVC(random_state=5))\n",
    "svc_clf_params = {\n",
    "    \"svc__C\": np.arange(0.1, 1, 0.1), \n",
    "    \"svc__kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "    \"svc__degree\": np.arange(3, 10, 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datasets(dataframe):\n",
    "    jobs_below_1200ms = dataframe.loc[dataframe[\"execTimeMs\"] < 1200]\n",
    "    jobs_between_2000ms_25000ms = dataframe.loc[dataframe[\"execTimeMs\"].between(2000, 25000)]\n",
    "    jobs_count = dataframe[\"name\"].value_counts()\n",
    "    jobs_most_occuring = dataframe.loc[dataframe[\"name\"].isin(jobs_count[jobs_count > 3000].index.values)]\n",
    "    jobs_mDiffFit = dataframe.loc[dataframe[\"name\"] == \"mDiffFit\"]\n",
    "    jobs_haplotype = dataframe.loc[dataframe[\"name\"] == \"haplotype_caller\"]\n",
    "    jobs_mShrink = dataframe.loc[dataframe[\"name\"] == \"mShrink\"]\n",
    "    return jobs_below_1200ms, jobs_between_2000ms_25000ms, jobs_most_occuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = make_datasets(load_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_for_jobs = [pd.DataFrame(y) for x, y in load_data().groupby('name', as_index=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_data(features, targets, regressors, verbose=10, pipeline_steps=base_steps):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.3, random_state=0)\n",
    "    df = pd.DataFrame(columns=[\"name\", \"pca\", \"adjusted_r2\",\"r2\", \"mae\", \"mape\", \"rae\", \"best_score\", \"params\"])\n",
    "    for (regressor, params) in regressors:\n",
    "        result = rate_regressor(X_train, y_train, X_test, y_test, regressor, params, verbose, pipeline_steps)\n",
    "        df = df.append({\"name\": regressor[0], **result, \"pca\": result[\"params\"][\"pca__n_components\"]}, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_data_explicit(X_train, X_test, y_train, y_test, regressors, verbose=10, pipeline_steps=base_steps):\n",
    "    df = pd.DataFrame(columns=[\"name\", \"pca\", \"adjusted_r2\",\"r2\", \"mae\", \"mape\", \"best_score\", \"params\"])\n",
    "    for (regressor, params) in regressors:\n",
    "        result = rate_regressor(X_train, y_train, X_test, y_test, regressor, params, verbose, pipeline_steps)\n",
    "        df = df.append({\"name\": regressor[0], **result, \"pca\": result[\"params\"][\"pca__n_components\"]}, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_classifiers_for_data(features, targets, classifiers, verbose=10, pipeline_steps=base_steps):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.3, random_state=0)\n",
    "    df = pd.DataFrame(columns=[\"name\", \"pca\", \"accuracy\",\"balanced_accuracy\", \"f1_micro\", \"f1_macro\", \"params\"])\n",
    "    for (classifier, params) in classifiers:\n",
    "        result = rate_classifier(X_train, y_train, X_test, y_test, classifier, params, verbose, pipeline_steps)\n",
    "        df = df.append({\"name\": classifier[0], **result, \"pca\": result[\"params\"][\"pca__n_components\"]}, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_dataset(dataframe, regressors, verbose=2):\n",
    "    print(f\"Rating dataset of len {len(dataframe)}\")\n",
    "    features, targets, _ = prepare_dataframe(dataframe[:10000])\n",
    "    features = preprocessor.fit_transform(features)\n",
    "    rate_data(features, targets, regressors, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_regressors = [\n",
    "    (knn, knn_params),\n",
    "#     (dtr, dtr_params),\n",
    "    (lasso, lasso_params),\n",
    "    (en, en_params),\n",
    "    (svr, svr_params),\n",
    "]\n",
    "\n",
    "basic_classifiers = [\n",
    "    (knn_classifier, knn_clf_params),\n",
    "#     (dtr_classifier, dtr_clf_params),\n",
    "    (mlp_classifier, mlp_clf_params),\n",
    "    (svc, svc_clf_params)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_experiment():\n",
    "    print(\"Rating jobs datasets\")\n",
    "    for dataset in dfs_for_jobs:\n",
    "        print(dataset.iloc[0][\"name\"])\n",
    "        rate_dataset(dataset, basic_regressors)\n",
    "\n",
    "    print(\"Rating common datasets\")\n",
    "    for dataset in datasets:\n",
    "        rate_dataset(dataset, basic_regressors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rate_dataset(dfs_for_jobs[8], verbose=0)\n",
    "# rate_dataset(dfs_for_jobs[1], basic_regressors, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eksperyment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zmierzyć skuteczności najlepszych pipelinów dla każdego joba, zobaczyć czy warto schodzić w dół pod wzgledem błędów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = load_data().dropna(axis=\"columns\")\n",
    "raw_datasets = { x:pd.DataFrame(y) for x, y in full_df.groupby('name', as_index=False)}\n",
    "datasets_split = {x:train_test_split(df, random_state=0, train_size=0.75) for x,df in raw_datasets.items()}\n",
    "# big_df_train, big_df_test = pd.concat([train for (_, (train, test)) in datasets_split.items()]), pd.concat([test for (_, (train, test)) in datasets_split.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_resource_features(features, dropped, baseline, resources):\n",
    "    for resource in resources:\n",
    "        features[resource] = dropped[resource].map(lambda value: calculate_quantile_rank(baseline[resource], value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numerical_pipeline_data(features, labels, dropped, baseline, resources=[\"read_sum\", \"write_sum\", \"cpu_mean\", \"memory_max\"]):\n",
    "    \"\"\"\n",
    "    Raw data enhanced with resource utilization quantile scores, but scores are assigned - not predicted\n",
    "    \"\"\"\n",
    "    add_resource_features(features, dropped, baseline, resources)\n",
    "    preprocessor = make_regression_preprocessor(resources)\n",
    "    features = preprocessor.fit_transform(features)\n",
    "    return features, labels, dropped, preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numerical_pipeline_data_big(data, resources=[\"read_sum\", \"write_sum\", \"cpu_mean\", \"memory_max\"]):\n",
    "    \"\"\"\n",
    "    Raw data enhanced with resource utilization quantile scores, but scores are assigned - not predicted\n",
    "    \"\"\"\n",
    "    features, labels, dropped = prepare_dataframe(data)\n",
    "    for resource in resources:\n",
    "        features[resource] = dropped[resource].map(lambda value: calculate_quantile_rank(dropped[resource], value))\n",
    "    features = make_regression_preprocessor(resources).fit_transform(features)\n",
    "    return pd.DataFrame(features, index=labels.index), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobId</th>\n",
       "      <th>read_sum</th>\n",
       "      <th>write_sum</th>\n",
       "      <th>readSyscalls_sum</th>\n",
       "      <th>writeSyscalls_sum</th>\n",
       "      <th>readReal_sum</th>\n",
       "      <th>writeReal_sum</th>\n",
       "      <th>writeCancelled_sum</th>\n",
       "      <th>rxBytes_sum</th>\n",
       "      <th>rxPackets_sum</th>\n",
       "      <th>...</th>\n",
       "      <th>mem.used</th>\n",
       "      <th>mem.active</th>\n",
       "      <th>mem.available</th>\n",
       "      <th>mem.buffers</th>\n",
       "      <th>mem.cached</th>\n",
       "      <th>mem.slab</th>\n",
       "      <th>mem.buffcache</th>\n",
       "      <th>mem.swaptotal</th>\n",
       "      <th>mem.swapused</th>\n",
       "      <th>mem.swapfree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15540</th>\n",
       "      <td>5VzTEh48X-1-8</td>\n",
       "      <td>409532.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32768.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>13633937408</td>\n",
       "      <td>5642911744</td>\n",
       "      <td>27716104192</td>\n",
       "      <td>2768896</td>\n",
       "      <td>6641188864</td>\n",
       "      <td>1846169600</td>\n",
       "      <td>8490127360</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86402</th>\n",
       "      <td>YrmwF3UAW-1-320</td>\n",
       "      <td>448476.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>359.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32768.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14300.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6938828800</td>\n",
       "      <td>3055312896</td>\n",
       "      <td>13768790016</td>\n",
       "      <td>315117568</td>\n",
       "      <td>3270520832</td>\n",
       "      <td>713506816</td>\n",
       "      <td>4299145216</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58973</th>\n",
       "      <td>yXB4dcCCr-1-34</td>\n",
       "      <td>241751.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>28672.0</td>\n",
       "      <td>32768.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1270759424</td>\n",
       "      <td>788201472</td>\n",
       "      <td>7468998656</td>\n",
       "      <td>454656</td>\n",
       "      <td>577638400</td>\n",
       "      <td>177721344</td>\n",
       "      <td>755814400</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110321</th>\n",
       "      <td>uwu6POi40-1-8</td>\n",
       "      <td>426355.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>316.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32768.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9407356928</td>\n",
       "      <td>1177980928</td>\n",
       "      <td>15646105600</td>\n",
       "      <td>97316864</td>\n",
       "      <td>8047538176</td>\n",
       "      <td>480485376</td>\n",
       "      <td>8625340416</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41349</th>\n",
       "      <td>bcKUGiUAf-1-112</td>\n",
       "      <td>428460.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>321.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32768.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11516.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10568097792</td>\n",
       "      <td>3031588864</td>\n",
       "      <td>30327427072</td>\n",
       "      <td>2768896</td>\n",
       "      <td>6220783616</td>\n",
       "      <td>1854623744</td>\n",
       "      <td>8078176256</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126832</th>\n",
       "      <td>NibmYG_N3-1-34</td>\n",
       "      <td>446548.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32768.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13686.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8767651840</td>\n",
       "      <td>1390714880</td>\n",
       "      <td>14376423424</td>\n",
       "      <td>342212608</td>\n",
       "      <td>6592135168</td>\n",
       "      <td>897110016</td>\n",
       "      <td>7831457792</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58434</th>\n",
       "      <td>ySTPaOAXY-1-164</td>\n",
       "      <td>316111.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32768.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3617013760</td>\n",
       "      <td>1105604608</td>\n",
       "      <td>7151595520</td>\n",
       "      <td>2768896</td>\n",
       "      <td>2224361472</td>\n",
       "      <td>593780736</td>\n",
       "      <td>2820911104</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83370</th>\n",
       "      <td>P2WYkNQ_Y-1-8</td>\n",
       "      <td>469391.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32768.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13622.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8932556800</td>\n",
       "      <td>3431043072</td>\n",
       "      <td>13393043456</td>\n",
       "      <td>123502592</td>\n",
       "      <td>5412884480</td>\n",
       "      <td>364777472</td>\n",
       "      <td>5901164544</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74809</th>\n",
       "      <td>DayfX-Qsv-1-86</td>\n",
       "      <td>414077.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>308.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32768.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8723546112</td>\n",
       "      <td>1424261120</td>\n",
       "      <td>15399841792</td>\n",
       "      <td>340754432</td>\n",
       "      <td>6542303232</td>\n",
       "      <td>907046912</td>\n",
       "      <td>7790104576</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110369</th>\n",
       "      <td>v1v9LgqQ6-1-34</td>\n",
       "      <td>327614.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>253.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32768.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5919141888</td>\n",
       "      <td>1399324672</td>\n",
       "      <td>15424778240</td>\n",
       "      <td>340844544</td>\n",
       "      <td>3767300096</td>\n",
       "      <td>904417280</td>\n",
       "      <td>5012561920</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  jobId  read_sum  write_sum  readSyscalls_sum  \\\n",
       "15540     5VzTEh48X-1-8  409532.0        9.0             305.0   \n",
       "86402   YrmwF3UAW-1-320  448476.0        9.0             359.0   \n",
       "58973    yXB4dcCCr-1-34  241751.0        9.0             192.0   \n",
       "110321    uwu6POi40-1-8  426355.0        9.0             316.0   \n",
       "41349   bcKUGiUAf-1-112  428460.0        9.0             321.0   \n",
       "...                 ...       ...        ...               ...   \n",
       "126832   NibmYG_N3-1-34  446548.0       19.0             356.0   \n",
       "58434   ySTPaOAXY-1-164  316111.0        9.0             247.0   \n",
       "83370     P2WYkNQ_Y-1-8  469391.0       19.0             391.0   \n",
       "74809    DayfX-Qsv-1-86  414077.0        9.0             308.0   \n",
       "110369   v1v9LgqQ6-1-34  327614.0        9.0             253.0   \n",
       "\n",
       "        writeSyscalls_sum  readReal_sum  writeReal_sum  writeCancelled_sum  \\\n",
       "15540                 9.0           0.0        32768.0                 0.0   \n",
       "86402                 9.0           0.0        32768.0                 0.0   \n",
       "58973                 9.0       28672.0        32768.0                 0.0   \n",
       "110321                9.0           0.0        32768.0                 0.0   \n",
       "41349                 9.0           0.0        32768.0                 0.0   \n",
       "...                   ...           ...            ...                 ...   \n",
       "126832               10.0           0.0        32768.0                 0.0   \n",
       "58434                 9.0           0.0        32768.0                 0.0   \n",
       "83370                10.0           0.0        32768.0                 0.0   \n",
       "74809                 9.0           0.0        32768.0                 0.0   \n",
       "110369                9.0           0.0        32768.0                 0.0   \n",
       "\n",
       "        rxBytes_sum  rxPackets_sum  ...     mem.used  mem.active  \\\n",
       "15540           0.0            0.0  ...  13633937408  5642911744   \n",
       "86402       14300.0           60.0  ...   6938828800  3055312896   \n",
       "58973           0.0            0.0  ...   1270759424   788201472   \n",
       "110321          0.0            0.0  ...   9407356928  1177980928   \n",
       "41349       11516.0           29.0  ...  10568097792  3031588864   \n",
       "...             ...            ...  ...          ...         ...   \n",
       "126832      13686.0           46.0  ...   8767651840  1390714880   \n",
       "58434           0.0            0.0  ...   3617013760  1105604608   \n",
       "83370       13622.0           51.0  ...   8932556800  3431043072   \n",
       "74809           0.0            0.0  ...   8723546112  1424261120   \n",
       "110369          0.0            0.0  ...   5919141888  1399324672   \n",
       "\n",
       "        mem.available  mem.buffers  mem.cached    mem.slab  mem.buffcache  \\\n",
       "15540     27716104192      2768896  6641188864  1846169600     8490127360   \n",
       "86402     13768790016    315117568  3270520832   713506816     4299145216   \n",
       "58973      7468998656       454656   577638400   177721344      755814400   \n",
       "110321    15646105600     97316864  8047538176   480485376     8625340416   \n",
       "41349     30327427072      2768896  6220783616  1854623744     8078176256   \n",
       "...               ...          ...         ...         ...            ...   \n",
       "126832    14376423424    342212608  6592135168   897110016     7831457792   \n",
       "58434      7151595520      2768896  2224361472   593780736     2820911104   \n",
       "83370     13393043456    123502592  5412884480   364777472     5901164544   \n",
       "74809     15399841792    340754432  6542303232   907046912     7790104576   \n",
       "110369    15424778240    340844544  3767300096   904417280     5012561920   \n",
       "\n",
       "        mem.swaptotal  mem.swapused  mem.swapfree  \n",
       "15540               0             0             0  \n",
       "86402               0             0             0  \n",
       "58973               0             0             0  \n",
       "110321              0             0             0  \n",
       "41349               0             0             0  \n",
       "...               ...           ...           ...  \n",
       "126832              0             0             0  \n",
       "58434               0             0             0  \n",
       "83370               0             0             0  \n",
       "74809               0             0             0  \n",
       "110369              0             0             0  \n",
       "\n",
       "[122 rows x 58 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_split['add_replace'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przebieg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trenujemy pipeline ogólny, liczymy jego skuteczności dla każdego typu jobów. Z eksperymentu 1. - najlepszy pipeline to był:\n",
    "\n",
    "dtr, mae, pca 71\n",
    "\n",
    "dla każdego typu jobów trenujemy dla niego pipeline, liczymy skuteczności"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = datasets_split['add_replace']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment4():\n",
    "    exp4_resources = [\"read_sum\", \"write_sum\", \"cpu_max\", \"cpu_mean\", \"memory_mean\", \"memory_max\"]\n",
    "    \n",
    "    def run_pipeline(name, features, targets):\n",
    "        pipeline_df = rate_data(features, targets, basic_regressors, verbose=10)\n",
    "        pipeline_df[\"pipeline\"] = name\n",
    "        return pipeline_df\n",
    "    \n",
    "    big_regressor = Pipeline([ # test pipeline\n",
    "        ('pca', PCA(random_state=42, n_components=31)),\n",
    "        ('knn', KNeighborsRegressor(n_neighbors=11))\n",
    "    ])\n",
    "    print(\"Preparing data for big regressor\")\n",
    "    X, y = get_numerical_pipeline_data_big(full_df, exp4_resources)\n",
    "    train_indices = np.concatenate([train.index for (_, (train, _)) in datasets_split.items()])\n",
    "    test_indices = np.concatenate([test.index for (_, (_, test)) in datasets_split.items()])\n",
    "    print(f\"{len(X)} {len(y)} {len(train_indices)} {len(test_indices)}\")\n",
    "    X_train, X_test, y_train, y_test = X.loc[train_indices], X.loc[test_indices], y.loc[train_indices], y.loc[test_indices]\n",
    "    print(f\"Making split with test as {len(test_indices)/(len(test_indices) + len(train_indices))} of dataset\")\n",
    "    \n",
    "    print(\"Training big regressor with train data\")\n",
    "    big_regressor.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Predicting with big regressor on test data\")\n",
    "    y_predicted = big_regressor.predict(X_test)\n",
    "    \n",
    "    print(\"Rating big regressor's overall performance\")\n",
    "    [r2, mae, mape, rae_score] = calculate_regression_score(y_test, y_predicted, [r2_score, mean_absolute_error, mean_absolute_percentage_error, rae])\n",
    "    print(f\"Scores for big regressor:\")\n",
    "    print(f\"R2: {r2}\")\n",
    "    print(f\"MAE: {mae}\")\n",
    "    print(f\"MAPE: {mape}\")\n",
    "    print(f\"RAE: {rae_score}\")\n",
    "    print()\n",
    "    dataframes = []\n",
    "    for (job, (train, test)) in datasets_split.items():\n",
    "        print(f\"Comparing big regressor vs local regressor for job {job}\")\n",
    "        print(\"Preparing data for local regressor\")\n",
    "        joint_df = pd.concat([train, test])\n",
    "        local_X, local_y = get_numerical_pipeline_data_big(joint_df, exp4_resources)\n",
    "        print(f\"{len(local_X)} {len(train.index)} {len(test.index)}\")\n",
    "        local_X_train, local_X_test, local_y_train, local_y_test = local_X.loc[train.index], local_X.loc[test.index], local_y.loc[train.index], local_y.loc[test.index]\n",
    "        \n",
    "        print(f\"Rating local regressors for job {job}\")\n",
    "        regressor_df = rate_data_explicit(local_X_train, local_X_test, local_y_train, local_y_test, basic_regressors, verbose=0)\n",
    "        print(regressor_df.head())\n",
    "        print(\"Preparing data for big regressor\")\n",
    "        local_X_test, local_y_test = X.loc[test_indices], y.loc[test_indices]\n",
    "        \n",
    "        print(f\"Rating big regressor for job {job}\")\n",
    "        local_predicted = big_regressor.predict(local_X_test, local_y_test)\n",
    "        [r2, mae, mape, rae_score] = calculate_regression_score(local_predicted, local_y_test, [r2_score, mean_absolute_error, mean_absolute_percentage_error, rae])\n",
    "        regressor_df = regressor_df.append({\"name\": \"big\", **{\"r2\": r2, \"mae\": mae, \"mape\": mape, \"rae\": rae_score}, \"pca\": \"xD\"}, ignore_index=True)\n",
    "        regressor_df[\"job\"] = job\n",
    "        regressor_df[\"size\"] = len(joint_df)\n",
    "        dataframes.append(regressor_df)\n",
    "        \n",
    "    return pd.concat(dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp4_df = run_experiment4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wyniki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykresy:\n",
    "- najlepsza skuteczność regresora (min mape/rae) vs liczba sampli\n",
    "- skuteczności 5 najlepszych regresorów dla każdego (grid chart)\n",
    "- najlepsza skuteczność regresora dla joba vs skuteczność dużego regresora dla tego joba\n",
    "\n",
    "Odpowiedzi:\n",
    "- czy zwiększenie granularności jest sensowne?\n",
    "- czy jest jakiś widoczny próg liczby sampli przy zwiększonej granularności?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
