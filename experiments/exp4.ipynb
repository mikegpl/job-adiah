{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to build sklearn-like pipeline for transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd         \n",
    "import os.path\n",
    "\n",
    "N_JOBS = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path=\"../data/csv/all.csv\"):\n",
    "    dataframe = pd.read_csv(path, index_col=0)\n",
    "    return dataframe.loc[~dataframe[\"execTimeMs\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataframe(dataframe):\n",
    "    output = dataframe.dropna(axis=\"columns\")\n",
    "    targets = output[\"execTimeMs\"]\n",
    "    dropped = output[[\"command\", \"execTimeMs\", \"jobId\", \"ctime_mean\", \"ctime_max\", \"ctime_sum\", \"read_sum\",\"write_sum\",\"readSyscalls_sum\",\"writeSyscalls_sum\",\"readReal_sum\",\"writeReal_sum\",\"writeCancelled_sum\",\"rxBytes_sum\",\"rxPackets_sum\",\"rxErrors_sum\",\"rxDrop_sum\",\"rxFifo_sum\",\"rxFrame_sum\",\"rxCompressed_sum\",\"rxMulticast_sum\",\"txBytes_sum\",\"txPackets_sum\",\"txErrors_sum\",\"txDrop_sum\",\"txFifo_sum\",\"txColls_sum\",\"txCarrier_sum\",\"txCompressed_sum\",\"cpu_mean\",\"cpu_max\",\"memory_mean\",\"memory_max\"]]\n",
    "    features = output.drop(dropped.columns, axis=1)\n",
    "    return features, targets, dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, targets, dropped = prepare_dataframe(load_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "workflowName          object\n",
       "size                 float64\n",
       "executable            object\n",
       "args                  object\n",
       "inputs                object\n",
       "outputs               object\n",
       "name                  object\n",
       "cpu.manufacturer      object\n",
       "cpu.brand             object\n",
       "cpu.speed            float64\n",
       "cpu.cores              int64\n",
       "cpu.physicalCores      int64\n",
       "cpu.processors         int64\n",
       "mem.total              int64\n",
       "mem.free               int64\n",
       "mem.used               int64\n",
       "mem.active             int64\n",
       "mem.available          int64\n",
       "mem.buffers            int64\n",
       "mem.cached             int64\n",
       "mem.slab               int64\n",
       "mem.buffcache          int64\n",
       "mem.swaptotal          int64\n",
       "mem.swapused           int64\n",
       "mem.swapfree           int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_selector as selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_list(series):\n",
    "    def vectorize(list_string):\n",
    "        return len(eval(list_string))\n",
    "    return np.vectorize(vectorize)(series)\n",
    "\n",
    "def ListTransformer():\n",
    "    return FunctionTransformer(func=vectorize_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_transformer = Pipeline(steps=[(\"list\", ListTransformer()), (\"scaler\", StandardScaler())])\n",
    "list_features = list(['args', 'inputs', 'outputs'])\n",
    "\n",
    "numerical_transformer = StandardScaler()\n",
    "numerical_features = list(features.select_dtypes(include=\"number\").columns)\n",
    "\n",
    "categorical_transformer = OneHotEncoder(sparse=False, handle_unknown = \"ignore\")\n",
    "categorical_features = list(set(features.select_dtypes(include=\"object\").columns) ^ set(list_features))\n",
    "\n",
    "def make_classifying_preprocessor(additional_features=[\"read_sum\", \"write_sum\", \"cpu_mean\", \"memory_max\"]):\n",
    "    external_features = categorical_features + additional_features\n",
    "    return ColumnTransformer(\n",
    "            transformers=[('lists', list_transformer, list_features), \n",
    "                          ('num', numerical_transformer, numerical_features),\n",
    "                          ('cat', categorical_transformer, external_features)])\n",
    "\n",
    "def make_regression_preprocessor(additional_features=[\"read_sum\", \"write_sum\", \"cpu_mean\", \"memory_max\"]):\n",
    "    external_features = numerical_features + additional_features\n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('lists', list_transformer, list_features),            \n",
    "            ('num', numerical_transformer, external_features),  \n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "\n",
    "preprocessor = make_classifying_preprocessor(additional_features=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import percentileofscore\n",
    "import math\n",
    "\n",
    "def calculate_quantile_rank(labels, label):\n",
    "    return percentileofscore(labels, label) / 100\n",
    "\n",
    "def calculate_utilization_class(labels, label):\n",
    "    def label_for_rank(rank):\n",
    "        if rank > 0.75:\n",
    "            return 'very high'\n",
    "        elif rank > 0.5:\n",
    "            return 'high'\n",
    "        elif rank > 0.25:\n",
    "            return 'medium'\n",
    "        else:\n",
    "            return 'low'\n",
    "    return label_for_rank(calculate_quantile_rank(labels, label))\n",
    "\n",
    "def calculate_utilization_bucket(labels, label, num_buckets):\n",
    "    bucket_size = 1.0 / num_buckets\n",
    "    def bucket_for_rank(rank):\n",
    "        return str(math.floor(rank / bucket_size))\n",
    "    return bucket_for_rank(calculate_quantile_rank(labels, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline composition (with PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.linear_model import Lasso, SGDRegressor, ElasticNet, LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, HalvingGridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_steps = [('pca', PCA(random_state=42))]\n",
    "dummy_pipeline = Pipeline(steps=base_steps +[('dummy', DummyRegressor())])\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_param_grid = {\n",
    "    'pca__n_components': np.arange(1, 50, 3),    \n",
    "}\n",
    "knn_param_grid = {\n",
    "    'knn__n_neighbors': np.arange(1, 30, 3),\n",
    "}\n",
    "regressor = ('knn', KNeighborsRegressor())\n",
    "full_pipeline = Pipeline(steps= base_steps + [regressor])\n",
    "grid_search = HalvingGridSearchCV(full_pipeline, {**knn_param_grid, **pca_param_grid}, cv=2, verbose=2, scoring=\"r2\", n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "from loky import get_reusable_executor\n",
    "\n",
    "def rate_regressor(X_train, y_train, X_test, y_test, regressor, regressor_params, verbose=10, aggressive_elimination=True, steps=base_steps):\n",
    "    print(f\"Rating {regressor}\")\n",
    "    full_pipeline = Pipeline(steps= base_steps + [regressor])\n",
    "    vector_length = X_train.shape[1]\n",
    "    pca_param_grid = {'pca__n_components': np.arange(1, vector_length, 1),}\n",
    "    grid_search = HalvingGridSearchCV(full_pipeline, {**pca_param_grid, **regressor_params}, cv=2, verbose=verbose, scoring=\"r2\", n_jobs=N_JOBS)\n",
    "    print(\"Evaluating grid search\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # scores\n",
    "    print(\"Predicting on test set\")\n",
    "    prediction = grid_search.best_estimator_.predict(X_test)\n",
    "    \n",
    "    print(\"Calculating scores\")\n",
    "    executor = get_reusable_executor(max_workers=3, timeout=5)\n",
    "    \n",
    "    scores = [lambda true, pred: r2_score(true, pred), lambda true, pred: mean_absolute_error(true, pred), lambda true, pred: mean_absolute_percentage_error(true, pred)]\n",
    "    results = executor.map(lambda fun: fun(y_test, prediction), scores)\n",
    "    print(\"Calculated scores on test set\")\n",
    "    r2, mae, mape = list(results)\n",
    "    adjusted_r2 = 1 - (1-r2)*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)\n",
    "    return {\"r2\": r2, \"adjusted_r2\": adjusted_r2, \"mae\": mae, \"mape\": mape,\"best_score\": grid_search.best_score_, \"params\": grid_search.best_params_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score, balanced_accuracy_score, roc_auc_score\n",
    "\n",
    "def rate_classifier(X_train, y_train, X_test, y_test, classifier, classifier_params, verbose=10, aggressive_elimination=True, steps=base_steps):\n",
    "    print(f\"Rating {classifier}\")\n",
    "    full_pipeline = Pipeline(steps= base_steps + [classifier])\n",
    "    vector_length = X_train.shape[1]\n",
    "    pca_param_grid = {'pca__n_components': np.arange(1, vector_length, 1),}\n",
    "    grid_search = HalvingGridSearchCV(full_pipeline, {**pca_param_grid, **classifier_params}, cv=2, verbose=verbose, scoring=\"accuracy\", n_jobs=N_JOBS)\n",
    "    print(\"Evaluating grid search\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # scores\n",
    "    print(\"Predicting on test set\")\n",
    "    prediction = grid_search.best_estimator_.predict(X_test)\n",
    "    \n",
    "    print(\"Calculating scores\")\n",
    "    executor = get_reusable_executor(max_workers=5, timeout=5)\n",
    "    \n",
    "    scores = [\n",
    "        lambda true, pred: f1_score(true, pred, average=\"micro\"),\n",
    "        lambda true, pred: f1_score(true, pred, average=\"macro\"),\n",
    "        lambda true, pred: accuracy_score(true, pred), \n",
    "        lambda true, pred: balanced_accuracy_score(true, pred)\n",
    "    ]\n",
    "    results = executor.map(lambda fun: fun(y_test, prediction), scores)\n",
    "    print(\"Calculated scores on test set\")\n",
    "    micro, macro, accuracy, balanced_accuracy = list(results)\n",
    "    return {\"f1_micro\": micro, \"f1_macro\": macro, \"accuracy\": accuracy, \"balanced_accuracy\": balanced_accuracy, \"params\": grid_search.best_params_}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here go regressor params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = (\"knn\", KNeighborsRegressor())\n",
    "knn_params = {'knn__n_neighbors': np.arange(1, 30, 1)}\n",
    "\n",
    "dtr = (\"dtr\", DecisionTreeRegressor(random_state=5))\n",
    "dtr_params = {\"dtr__criterion\": [\"mse\", \"friedman_mse\", \"mae\", \"poisson\"]}\n",
    "\n",
    "lasso = (\"lasso\", Lasso(random_state=5))\n",
    "lasso_params = {\"lasso__alpha\": np.arange(0.01, 1, 0.05)}\n",
    "\n",
    "en = (\"elasticnet\", ElasticNet(random_state=5))\n",
    "en_params = {\"elasticnet__alpha\": np.arange(0.01, 1, 0.05), \"elasticnet__l1_ratio\": np.arange(0, 1, 0.1)}\n",
    "\n",
    "svr = (\"svr\", SGDRegressor())\n",
    "svr_params = {\"svr__loss\": [\"squared_loss\", \"huber\", \"epsilon_insensitive\"], \"svr__penalty\": ['l2', 'l1', 'elasticnet'],\n",
    "             \"svr__alpha\": np.arange(0.0001, 0.2, 0.01), \"svr__max_iter\": [10000]}\n",
    "\n",
    "rf = (\"rf\", RandomForestRegressor())\n",
    "rf_params = {\"rf__n_estimators\": np.arange(5, 100, 5), \"rf__criterion\": [\"mae\", \"mse\"], \"rf__max_features\": [\"auto\", \"sqrt\", \"log2\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here go classifier params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_classifier = (\"knn\", KNeighborsClassifier())\n",
    "knn_clf_params = {'knn__n_neighbors': np.arange(1, 30, 1)}\n",
    "\n",
    "dtr_classifier = (\"dtr\", DecisionTreeClassifier(random_state=5))\n",
    "dtr_clf_params = {\"dtr__criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "mlp_classifier = (\"mlp\", MLPClassifier())\n",
    "mlp_clf_params = {\"mlp__hidden_layer_sizes\": np.arange(1,200, 10),          \n",
    "                  \"mlp__activation\": [\"logistic\", \"tanh\", \"relu\"],         \n",
    "                  \"mlp__activation\": [\"logistic\"],         \n",
    "#                   \"mlp__alpha\": np.arange(0.01, 0.1, 0.01)\n",
    "                 }\n",
    "\n",
    "svc = (\"svc\", SVC(random_state=5))\n",
    "svc_clf_params = {\n",
    "    \"svc__C\": np.arange(0.1, 1, 0.1), \n",
    "    \"svc__kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"],\n",
    "    \"svc__degree\": np.arange(3, 10, 1)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datasets(dataframe):\n",
    "    jobs_below_1200ms = dataframe.loc[dataframe[\"execTimeMs\"] < 1200]\n",
    "    jobs_between_2000ms_25000ms = dataframe.loc[dataframe[\"execTimeMs\"].between(2000, 25000)]\n",
    "    jobs_count = dataframe[\"name\"].value_counts()\n",
    "    jobs_most_occuring = dataframe.loc[dataframe[\"name\"].isin(jobs_count[jobs_count > 3000].index.values)]\n",
    "    jobs_mDiffFit = dataframe.loc[dataframe[\"name\"] == \"mDiffFit\"]\n",
    "    jobs_haplotype = dataframe.loc[dataframe[\"name\"] == \"haplotype_caller\"]\n",
    "    jobs_mShrink = dataframe.loc[dataframe[\"name\"] == \"mShrink\"]\n",
    "    return jobs_below_1200ms, jobs_between_2000ms_25000ms, jobs_most_occuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = make_datasets(load_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_for_jobs = [pd.DataFrame(y) for x, y in load_data().groupby('name', as_index=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_data(features, targets, regressors, verbose=10, pipeline_steps=base_steps):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.3, random_state=0)\n",
    "    df = pd.DataFrame(columns=[\"name\", \"pca\", \"adjusted_r2\",\"r2\", \"mae\", \"mape\", \"best_score\", \"params\"])\n",
    "    for (regressor, params) in regressors:\n",
    "        result = rate_regressor(X_train, y_train, X_test, y_test, regressor, params, verbose, pipeline_steps)\n",
    "        df = df.append({\"name\": regressor[0], **result, \"pca\": result[\"params\"][\"pca__n_components\"]}, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_classifiers_for_data(features, targets, classifiers, verbose=10, pipeline_steps=base_steps):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.3, random_state=0)\n",
    "    df = pd.DataFrame(columns=[\"name\", \"pca\", \"accuracy\",\"balanced_accuracy\", \"f1_micro\", \"f1_macro\", \"params\"])\n",
    "    for (classifier, params) in classifiers:\n",
    "        result = rate_classifier(X_train, y_train, X_test, y_test, classifier, params, verbose, pipeline_steps)\n",
    "        df = df.append({\"name\": classifier[0], **result, \"pca\": result[\"params\"][\"pca__n_components\"]}, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_dataset(dataframe, regressors, verbose=2):\n",
    "    print(f\"Rating dataset of len {len(dataframe)}\")\n",
    "    features, targets, _ = prepare_dataframe(dataframe[:10000])\n",
    "    features = preprocessor.fit_transform(features)\n",
    "    rate_data(features, targets, regressors, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_regressors = [\n",
    "    (knn, knn_params),\n",
    "#     (dtr, dtr_params),\n",
    "    (lasso, lasso_params),\n",
    "    (en, en_params),\n",
    "    (svr, svr_params),\n",
    "]\n",
    "\n",
    "basic_classifiers = [\n",
    "    (knn_classifier, knn_clf_params),\n",
    "#     (dtr_classifier, dtr_clf_params),\n",
    "    (mlp_classifier, mlp_clf_params),\n",
    "    (svc, svc_clf_params)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_experiment():\n",
    "    print(\"Rating jobs datasets\")\n",
    "    for dataset in dfs_for_jobs:\n",
    "        print(dataset.iloc[0][\"name\"])\n",
    "        rate_dataset(dataset, basic_regressors)\n",
    "\n",
    "    print(\"Rating common datasets\")\n",
    "    for dataset in datasets:\n",
    "        rate_dataset(dataset, basic_regressors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rate_dataset(dfs_for_jobs[8], verbose=0)\n",
    "# rate_dataset(dfs_for_jobs[1], basic_regressors, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eksperyment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zmierzyć skuteczności najlepszych pipelinów dla każdego joba, zobaczyć czy warto schodzić w dół pod wzgledem błędów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataframe\n",
    "all_dataframe = exp1_datasets[\"All\"]\n",
    "raw_datasets = {x:pd.DataFrame(y) for x, y in raw_dataframe.groupby('name', as_index=False)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przebieg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trenujemy pipeline ogólny, liczymy jego skuteczności dla każdego typu jobów\n",
    "\n",
    "dla każdego typu jobów trenujemy dla niego pipeline, liczymy skuteczności\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wyniki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wykresy:\n",
    "- najlepsza skuteczność regresora (min mape/rae) vs liczba sampli\n",
    "- skuteczności 5 najlepszych regresorów dla każdego (grid chart)\n",
    "- najlepsza skuteczność regresora dla joba vs skuteczność dużego regresora dla tego joba\n",
    "\n",
    "Odpowiedzi:\n",
    "- czy zwiększenie granularności jest sensowne?\n",
    "- czy jest jakiś widoczny próg liczby sampli przy zwiększonej granularności?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
