{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "def read_jsonl(relative_path):\n",
    "    with open(relative_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    jsons = [json.loads(line) for line in lines]\n",
    "    return pd.json_normalize(jsons)\n",
    "\n",
    "def list_files_by_extension(extension, directory_path):\n",
    "    files = [file for file in os.listdir(directory_path) if file.lower().endswith(extension.lower())]\n",
    "    sorted_by_name = sorted(files)\n",
    "    return sorted_by_name\n",
    "\n",
    "def list_directores_in_path(relative_path):\n",
    "    return [os.path.join(relative_path, fname) for fname in os.listdir(relative_path) if os.path.isdir(os.path.join(relative_path, fname))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "\n",
    "def load_data_run(path):\n",
    "    print(f\"Loading from {path}\")\n",
    "    files = [os.path.join(path, file) for file in [\"job_descriptions.jsonl\", \"sys_info.jsonl\"]]\n",
    "    meta = [read_jsonl(file) for file in files]\n",
    "    events = read_jsonl(os.path.join(path, \"metrics.jsonl\"))\n",
    "    meta = pd.merge(meta[0], meta[1], on=\"jobId\")\n",
    "    return np.array([meta, events])\n",
    "\n",
    "def load_from_sources(paths):\n",
    "    frames = np.array([load_data_run(path) for path in paths])\n",
    "    return pd.concat(frames[:, 0]), pd.concat(frames[:, 1])\n",
    "\n",
    "def load_frames_for_base(path, base):\n",
    "    base_path = os.path.join(path,base)\n",
    "    categories = list_directores_in_path(base_path)\n",
    "    frames_paths =list(chain(*[list_directores_in_path(category) for category in categories]))\n",
    "    return load_from_sources(frames_paths[100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"../data/parsed-logs/\"\n",
    "\n",
    "aws_bases = list_directores_in_path(data_root + \"aws\")\n",
    "gcloud_bases = list_directores_in_path(data_root +\"gcloud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from ../data/parsed-logs/gcloud/n2d-standard-4/montage2__1.0__1.0.0__2020-06-20-21-44-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-34-8ee35826f115>:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array([meta, events])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from ../data/parsed-logs/gcloud/n2d-standard-4/soykb__56__1.0.0__2020-06-21-21-25-12\n",
      "Loading from ../data/parsed-logs/gcloud/n2d-standard-4/montage2__0.25__1.0.0__2020-06-20-20-15-27\n",
      "Loading from ../data/parsed-logs/gcloud/n2d-standard-4/montage__0.25__1.0.0__2020-06-21-14-10-27\n",
      "Loading from ../data/parsed-logs/gcloud/n2d-standard-4/montage__2.0__1.0.0__2020-06-21-17-04-44\n",
      "Loading from ../data/parsed-logs/gcloud/n2d-standard-4/soykb__238__1.0.0__2020-06-21-07-11-24\n",
      "Loading from ../data/parsed-logs/gcloud/n2d-standard-4/montage__0.25__1.0.0__2020-06-20-19-19-24\n",
      "Loading from ../data/parsed-logs/gcloud/n2d-standard-4/montage2__0.01__1.0.0__2020-06-21-17-17-48\n",
      "Loading from ../data/parsed-logs/gcloud/n2d-standard-4/soykb__82__1.0.0__2020-06-21-22-38-21\n",
      "Loading from ../data/parsed-logs/gcloud/n2d-standard-4/montage__1.0__1.0.0__2020-06-21-16-46-00\n",
      "Loading from ../data/parsed-logs/gcloud/n2d-standard-4/soykb__134__1.0.0__2020-06-22-00-37-22\n",
      "Loading from ../data/parsed-logs/gcloud/n2d-standard-4/soykb__82__1.0.0__2020-06-21-01-36-42\n",
      "Loading from ../data/parsed-logs/gcloud/n2d-standard-4/montage2__1.0__1.0.0__2020-06-21-18-43-46\n",
      "Loading from ../data/parsed-logs/gcloud/n2d-standard-4/soykb__134__1.0.0__2020-06-21-03-35-15\n",
      "Loading from ../data/parsed-logs/gcloud/n2d-standard-4/soykb__446__1.0.0__2020-06-21-14-04-41\n",
      "Loading from ../data/parsed-logs/gcloud/n2d-standard-4/montage__2.0__1.0.0__2020-06-20-19-53-33\n",
      "Loading from ../data/parsed-logs/gcloud/n2d-standard-4/montage__1.0__1.0.0__2020-06-20-19-33-55\n",
      "Loading from ../data/parsed-logs/gcloud/n2d-standard-4/montage2__0.25__1.0.0__2020-06-21-17-25-44\n",
      "Loading from ../data/parsed-logs/gcloud/n2d-standard-4/soykb__238__1.0.0__2020-06-22-11-18-29\n",
      "Loading from ../data/parsed-logs/gcloud/n2d-standard-4/soykb__446__1.0.0__2020-06-22-18-42-45\n",
      "Loading from ../data/parsed-logs/gcloud/n2d-standard-4/montage2__0.01__1.0.0__2020-06-20-20-07-12\n",
      "Loading from ../data/parsed-logs/gcloud/n2-standard-4/montage__2.0__1.0.0__2020-06-02-06-36-48\n",
      "Loading from ../data/parsed-logs/gcloud/n2-standard-4/soykb__446__1.0.0__2020-06-23-08-02-29\n",
      "Loading from ../data/parsed-logs/gcloud/n2-standard-4/montage__1.0__1.0.0__2020-06-02-06-19-03\n",
      "Loading from ../data/parsed-logs/gcloud/n2-standard-4/montage__0.25__1.0.0__2020-06-02-06-13-15\n",
      "Loading from ../data/parsed-logs/gcloud/n2-standard-4/soykb__238__1.0.0__2020-06-22-12-09-40\n",
      "Loading from ../data/parsed-logs/gcloud/n2-standard-4/soykb__134__1.0.0__2020-06-22-01-00-21\n",
      "Loading from ../data/parsed-logs/gcloud/n2-standard-4/soykb__82__1.0.0__2020-06-21-22-33-33\n",
      "Loading from ../data/parsed-logs/gcloud/n2-standard-4/soykb__56__1.0.0__2020-06-02-09-24-48\n",
      "Loading from ../data/parsed-logs/gcloud/n2-standard-4/montage2__0.01__1.0.0__2020-06-02-06-49-05\n",
      "Loading from ../data/parsed-logs/gcloud/n2-standard-4/soykb__134__1.0.0__2020-06-03-00-40-39\n",
      "Loading from ../data/parsed-logs/gcloud/n2-standard-4/montage2__1.0__1.0.0__2020-06-21-19-27-55\n",
      "Loading from ../data/parsed-logs/gcloud/n2-standard-4/montage__1.0__1.0.0__2020-06-21-17-39-57\n",
      "Loading from ../data/parsed-logs/gcloud/n2-standard-4/montage2__1.0__1.0.0__2020-06-02-07-53-32\n",
      "Loading from ../data/parsed-logs/gcloud/n2-standard-4/soykb__56__1.0.0__2020-06-21-21-05-48\n",
      "Loading from ../data/parsed-logs/gcloud/n2-standard-4/montage__0.25__1.0.0__2020-06-21-17-33-09\n",
      "Loading from ../data/parsed-logs/gcloud/n2-standard-4/montage2__0.25__1.0.0__2020-06-02-06-57-11\n",
      "Loading from ../data/parsed-logs/gcloud/n2-standard-4/montage__2.0__1.0.0__2020-06-21-17-59-46\n",
      "Loading from ../data/parsed-logs/gcloud/n2-standard-4/montage2__0.01__1.0.0__2020-06-21-18-13-47\n",
      "Loading from ../data/parsed-logs/gcloud/n2-standard-4/soykb__238__1.0.0__2020-06-03-05-36-23\n",
      "Loading from ../data/parsed-logs/gcloud/n2-standard-4/soykb__82__1.0.0__2020-06-02-10-51-11\n",
      "Loading from ../data/parsed-logs/gcloud/n2-standard-4/soykb__446__1.0.0__2020-06-04-02-55-26\n",
      "Loading from ../data/parsed-logs/gcloud/n2-standard-4/montage2__0.25__1.0.0__2020-06-21-18-21-41\n",
      "Loading from ../data/parsed-logs/gcloud/e2-medium/montage__0.25__1.0.0__2020-05-30-16-42-38\n",
      "Loading from ../data/parsed-logs/gcloud/e2-medium/montage__0.25__1.0.0__2020-05-30-16-58-46\n",
      "Loading from ../data/parsed-logs/gcloud/e2-standard-4/soykb__238__1.0.0__2020-05-29-21-17-37\n",
      "Loading from ../data/parsed-logs/gcloud/e2-standard-4/soykb__446__1.0.0__2020-06-02-23-24-58\n",
      "Loading from ../data/parsed-logs/gcloud/e2-standard-4/soykb__134__1.0.0__2020-06-02-02-01-57\n",
      "Loading from ../data/parsed-logs/gcloud/e2-standard-4/soykb__654__1.0.0__2020-06-01-08-15-07\n",
      "Loading from ../data/parsed-logs/gcloud/e2-standard-4/montage__0.25__1.0.0__2020-05-29-09-15-19\n",
      "Loading from ../data/parsed-logs/gcloud/e2-standard-4/soykb__134__1.0.0__2020-05-29-15-17-42\n",
      "Loading from ../data/parsed-logs/gcloud/e2-standard-4/montage__1.0__1.0.0__2020-06-01-08-30-21\n",
      "Loading from ../data/parsed-logs/gcloud/e2-standard-4/montage__1.0__1.0.0__2020-06-01-17-51-35\n",
      "Loading from ../data/parsed-logs/gcloud/e2-standard-4/soykb__238__1.0.0__2020-06-02-08-09-56\n",
      "Loading from ../data/parsed-logs/gcloud/e2-standard-4/soykb__56__1.0.0__2020-05-29-10-28-36\n",
      "Loading from ../data/parsed-logs/gcloud/e2-standard-4/soykb__56__1.0.0__2020-06-01-21-05-49\n",
      "Loading from ../data/parsed-logs/gcloud/e2-standard-4/montage2__0.25__1.0.0__2020-06-01-18-26-43\n",
      "Loading from ../data/parsed-logs/gcloud/e2-standard-4/montage__0.25__1.0.0__2020-06-01-08-24-24\n",
      "Loading from ../data/parsed-logs/gcloud/e2-standard-4/montage2__0.25__1.0.0__2020-06-01-09-02-36\n",
      "Loading from ../data/parsed-logs/gcloud/e2-standard-4/montage__2.0__1.0.0__2020-06-01-08-45-07\n",
      "Loading from ../data/parsed-logs/gcloud/e2-standard-4/montage2__1.0__1.0.0__2020-06-01-17-08-44\n",
      "Loading from ../data/parsed-logs/gcloud/e2-standard-4/soykb__446__1.0.0__2020-05-30-22-50-31\n",
      "Loading from ../data/parsed-logs/gcloud/e2-standard-4/soykb__82__1.0.0__2020-06-01-22-54-26\n",
      "Loading from ../data/parsed-logs/gcloud/e2-standard-4/montage2__1.0__1.0.0__2020-06-01-19-12-39\n",
      "Loading from ../data/parsed-logs/gcloud/e2-standard-4/soykb__82__1.0.0__2020-05-29-12-13-51\n",
      "Loading from ../data/parsed-logs/gcloud/e2-standard-4/montage__2.0__1.0.0__2020-06-01-18-08-23\n",
      "Loading from ../data/parsed-logs/gcloud/e2-standard-4/montage2__0.01__1.0.0__2020-05-31-09-50-39\n"
     ]
    }
   ],
   "source": [
    "df = load_frames_for_base(data_root, \"gcloud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta, events = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_events = events.loc[events[\"parameter\"] != \"event\"].reset_index()\n",
    "# filtered_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_events.parameter.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\"read\": 1225, \"write\": 1, \"readSyscalls\": 5, \"writeSyscalls\": 1, \"readReal\": 0, \"writeReal\": 0, \"writeCancelled\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_params = [\"read\", \"write\", \"readSyscalls\", \"writeSyscalls\", \"readReal\", \"writeReal\", \"writeCancelled\"]\n",
    "network_params = [\"rxBytes\", \"rxPackets\", \"rxErrors\", \"rxDrop\", \"rxFifo\", \"rxFrame\", \"rxCompressed\", \"rxMulticast\", \"txBytes\", \"txPackets\", \"txErrors\", \"txDrop\", \"txFifo\", \"txColls\", \"txCarrier\", \"txCompressed\"]\n",
    "\n",
    "io_agg_kwargs={f\"{param}_sum\" : (f\"value.{param}\", \"sum\") for param in io_params}\n",
    "network_agg_kwargs={f\"{param}_sum\" : (f\"value.{param}\", \"sum\") for param in network_params}\n",
    "cpu_agg_kwargs={\"cpu_mean\": ('value', 'mean'), \"cpu_max\": ('value', 'max')}\n",
    "memory_agg_kwargs={\"memory_mean\": ('value', 'mean'), \"memory_max\": ('value', 'max')}\n",
    "ctime_agg_kwargs={\"ctime_mean\": ('value', 'mean'), \"ctime_max\": ('value', 'max'), \"ctime_sum\": ('value', 'sum')}\n",
    "\n",
    "def extract_meta_from_metrics(frame):\n",
    "    frame[\"value\"] = pd.to_numeric(frame[\"value\"])\n",
    "    def postprocess(aggregated_frame, parameter_type):\n",
    "        aggregated_frame = aggregated_frame.reset_index()\n",
    "        return aggregated_frame.loc[aggregated_frame[\"parameter\"] == parameter_type].drop(\"parameter\", axis=1)\n",
    "    \n",
    "    frame_grp = frame.groupby(['jobId', 'parameter'])\n",
    "    metrics = [\n",
    "        postprocess(frame_grp.agg(**io_agg_kwargs), \"io\"),\n",
    "        postprocess(frame_grp.agg(**network_agg_kwargs), \"network\"),          \n",
    "        postprocess(frame_grp.agg(**cpu_agg_kwargs), \"cpu\"),\n",
    "        postprocess(frame_grp.agg(**memory_agg_kwargs), \"memory\"),\n",
    "        postprocess(frame_grp.agg(**ctime_agg_kwargs), \"ctime\")   \n",
    "    ]   \n",
    "    return reduce(lambda left, right: pd.merge(left,right,on='jobId'), metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_meta_from_metrics(filtered_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.rcParams['figure.figsize'] = [10, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta.groupby('executable').count().sort_values('size').plot.barh(y='size', logx=True, legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo - zmerge'ować base z metadanymi. Zrobić matrycę pearsona. Podstawowe klasyfikatory. GPT-2. Podział na "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "workflowName                                                      montage2\n",
       "size                                                                   1.0\n",
       "version                                                              1.0.0\n",
       "hyperflowId                                                      0EUZ6gtGK\n",
       "jobId                                                     0EUZ6gtGK-1-4049\n",
       "nodeName                                gke-cluster-x-worker-f40a1baf-vlcx\n",
       "executable                                                        mDiffFit\n",
       "args                     [-d, -s, 3-fit.000068.000103.txt, p2mass-atlas...\n",
       "inputs                   [{'name': 'region-oversized.hdr', 'size': 277}...\n",
       "outputs                  [{'name': '3-fit.000068.000103.txt', 'size': 2...\n",
       "name                                                              mDiffFit\n",
       "command                  mDiffFit -d -s 3-fit.000068.000103.txt p2mass-...\n",
       "execTimeMs                                                             415\n",
       "env.podIp                                                      10.24.0.142\n",
       "env.nodeName                            gke-cluster-x-worker-f40a1baf-vlcx\n",
       "env.podName                                job0pwsi3-mdifffit-4049-1-kr2xk\n",
       "env.podServiceAccount                                              default\n",
       "env.podNamespace                                                   default\n",
       "cpu.manufacturer                                                       AMD\n",
       "cpu.brand                                                        EPYC 7B12\n",
       "cpu.vendor                                                                \n",
       "cpu.family                                                                \n",
       "cpu.model                                                                 \n",
       "cpu.stepping                                                              \n",
       "cpu.revision                                                              \n",
       "cpu.voltage                                                               \n",
       "cpu.speed                                                             2.25\n",
       "cpu.speedmin                                                              \n",
       "cpu.speedmax                                                              \n",
       "cpu.governor                                                              \n",
       "cpu.cores                                                                4\n",
       "cpu.physicalCores                                                        4\n",
       "cpu.processors                                                           1\n",
       "cpu.socket                                                                \n",
       "cpu.cache.l1d                                                             \n",
       "cpu.cache.l1i                                                             \n",
       "cpu.cache.l2                                                              \n",
       "cpu.cache.l3                                                              \n",
       "mem.total                                                      16824102912\n",
       "mem.free                                                        6968659968\n",
       "mem.used                                                        9855442944\n",
       "mem.active                                                      1414307840\n",
       "mem.available                                                  15409795072\n",
       "mem.buffers                                                      314077184\n",
       "mem.cached                                                      7743676416\n",
       "mem.slab                                                         843911168\n",
       "mem.buffcache                                                   8901664768\n",
       "mem.swaptotal                                                            0\n",
       "mem.swapused                                                             0\n",
       "mem.swapfree                                                             0\n",
       "stdout                                                                 NaN\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.iloc[0]\n",
    "# meta.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_metrics(events_frame):\n",
    "    print(\"Preprocessing metrics\")\n",
    "    events_frame = events_frame.loc[events_frame[\"parameter\"] != \"event\"].reset_index()\n",
    "    return extract_meta_from_metrics(events_frame)\n",
    "\n",
    "def preprocess_meta(meta_frame):\n",
    "    print(\"Preprocessing meta\")\n",
    "    return meta_frame.drop([\"hyperflowId\", 'version', 'nodeName', 'cpu.socket', 'cpu.speedmin', \n",
    "                            'cpu.speedmax', 'cpu.governor', 'cpu.revision', 'cpu.voltage', 'env.nodeName', \n",
    "                            'env.podIp', 'env.podServiceAccount', 'env.podName', 'env.podNamespace', 'stdout'], axis=1)\n",
    "\n",
    "def join_metrics_with_meta(metrics_frame, meta_frame):\n",
    "    print(\"Joining metrics with meta\")\n",
    "    return pd.merge(metrics_frame, meta_frame, on=\"jobId\")\n",
    "\n",
    "def dump_preprocessed(data_frame, path=\"merged.csv\"):\n",
    "    print(\"Dumping preprocessed\")\n",
    "    data_frame.to_csv(os.path.join(data_root, path))\n",
    "\n",
    "def vectorize_data(data_frame):\n",
    "    # inputs -> number of inputs\n",
    "    # outputs -> number of outputs\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_intermediate():\n",
    "    metrics = preprocess_metrics(events)\n",
    "    metadata = preprocess_meta(meta)\n",
    "    joint_df = join_metrics_with_meta(metrics, metadata)\n",
    "    dump_preprocessed(joint_df, \"googles150.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing metrics\n"
     ]
    }
   ],
   "source": [
    "store_intermediate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jobadiah",
   "language": "python",
   "name": "jobadiah"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
