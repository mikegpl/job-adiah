{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "def read_jsonl(relative_path):\n",
    "    with open(relative_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    jsons = [json.loads(line) for line in lines]\n",
    "    return pd.json_normalize(jsons)\n",
    "\n",
    "def list_files_by_extension(extension, directory_path):\n",
    "    files = [file for file in os.listdir(directory_path) if file.lower().endswith(extension.lower())]\n",
    "    sorted_by_name = sorted(files)\n",
    "    return sorted_by_name\n",
    "\n",
    "def list_directores_in_path(relative_path):\n",
    "    return [os.path.join(relative_path, fname) for fname in os.listdir(relative_path) if os.path.isdir(os.path.join(relative_path, fname))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "\n",
    "def load_data_run(path):\n",
    "    print(f\"Loading from {path}\")\n",
    "    files = [os.path.join(path, file) for file in [\"job_descriptions.jsonl\", \"sys_info.jsonl\"]]\n",
    "    meta = [read_jsonl(file) for file in files]\n",
    "    events = read_jsonl(os.path.join(path, \"metrics.jsonl\"))\n",
    "    meta = pd.merge(meta[0], meta[1], on=\"jobId\")\n",
    "    return np.array([meta, events])\n",
    "\n",
    "def load_from_sources(paths):\n",
    "    frames = np.array([load_data_run(path) for path in paths])\n",
    "    return pd.concat(frames[:, 0]), pd.concat(frames[:, 1])\n",
    "\n",
    "def load_frames_for_base(path, base):\n",
    "    base_path = os.path.join(path,base)\n",
    "    categories = list_directores_in_path(base_path)\n",
    "    frames_paths =list(chain(*[list_directores_in_path(category) for category in categories]))\n",
    "    return load_from_sources(frames_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"../data/parsed-logs/\"\n",
    "\n",
    "aws_bases = list_directores_in_path(data_root + \"aws\")\n",
    "gcloud_bases = list_directores_in_path(data_root +\"gcloud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from ../data/parsed-logs/aws/unassigned/montage__1.0__1.0.0__2020-04-28-22-02-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-d4b78fde1246>:11: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array([meta, events])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from ../data/parsed-logs/aws/unassigned/soykb__134__1.0.0__2020-04-29-04-29-50\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__2.0__1.0.0__2020-04-29-13-04-35\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__0.25__1.0.0__2020-04-29-09-24-06\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__0.25__1.0.0__2020-04-28-21-42-32\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__1.0__1.0.0__2020-04-29-10-48-16\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__0.25__1.0.0__2020-04-28-21-45-01\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__2.0__1.0.0__2020-04-29-12-01-28\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__2.0__1.0.0__2020-04-28-22-24-43\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__0.25__1.0.0__2020-04-29-09-29-33\n",
      "Loading from ../data/parsed-logs/aws/unassigned/soykb__56__1.0.0__2020-04-29-00-58-21\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__1.0__1.0.0__2020-04-29-10-15-49\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__2.0__1.0.0__2020-04-28-22-45-32\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__1.0__1.0.0__2020-04-29-10-56-10\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__0.25__1.0.0__2020-04-29-09-27-08\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__1.0__1.0.0__2020-04-29-09-59-22\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__1.0__1.0.0__2020-04-28-21-52-59\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__2.0__1.0.0__2020-04-28-23-06-54\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__0.25__1.0.0__2020-04-28-21-47-24\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__1.0__1.0.0__2020-04-28-22-10-28\n",
      "Loading from ../data/parsed-logs/aws/unassigned/soykb__56__1.0.0__2020-04-29-01-49-02\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__0.25__1.0.0__2020-04-29-09-41-35\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__2.0__1.0.0__2020-04-29-13-25-23\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__0.25__1.0.0__2020-04-29-09-39-09\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__1.0__1.0.0__2020-04-29-10-07-14\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__0.25__1.0.0__2020-04-29-09-44-00\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__0.25__1.0.0__2020-04-29-09-36-45\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__2.0__1.0.0__2020-04-29-14-06-54\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__2.0__1.0.0__2020-04-29-14-27-59\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__0.25__1.0.0__2020-04-29-09-31-56\n",
      "Loading from ../data/parsed-logs/aws/unassigned/soykb__134__1.0.0__2020-04-29-02-07-59\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__2.0__1.0.0__2020-04-29-12-22-03\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__1.0__1.0.0__2020-04-29-10-31-26\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__1.0__1.0.0__2020-04-29-11-05-14\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__0.25__1.0.0__2020-04-29-09-46-31\n",
      "Loading from ../data/parsed-logs/aws/unassigned/soykb__134__1.0.0__2020-04-29-04-09-27\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__1.0__1.0.0__2020-04-29-09-51-37\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__0.25__1.0.0__2020-04-28-21-18-41\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__1.0__1.0.0__2020-04-29-10-23-36\n",
      "Loading from ../data/parsed-logs/aws/unassigned/soykb__56__1.0.0__2020-04-29-00-09-21\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__0.25__1.0.0__2020-04-29-09-34-21\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__2.0__1.0.0__2020-04-29-12-43-13\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__2.0__1.0.0__2020-04-29-11-40-43\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__0.25__1.0.0__2020-04-29-08-59-29\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__2.0__1.0.0__2020-04-29-13-46-03\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__1.0__1.0.0__2020-04-29-10-40-01\n",
      "Loading from ../data/parsed-logs/aws/unassigned/montage__2.0__1.0.0__2020-04-29-11-20-05\n",
      "Loading from ../data/parsed-logs/aws/t3.large/montage__2.0__1.0.0__2020-06-01-18-00-55\n",
      "Loading from ../data/parsed-logs/aws/t3.large/montage2__0.25__1.0.0__2020-06-01-19-27-45\n",
      "Loading from ../data/parsed-logs/aws/t3.large/soykb__56__1.0.0__2020-06-21-02-40-48\n",
      "Loading from ../data/parsed-logs/aws/t3.large/soykb__82__1.0.0__2020-06-21-04-22-08\n",
      "Loading from ../data/parsed-logs/aws/t3.large/montage__2.0__1.0.0__2020-06-01-19-02-29\n",
      "Loading from ../data/parsed-logs/aws/t3.large/soykb__134__1.0.0__2020-06-21-07-18-37\n",
      "Loading from ../data/parsed-logs/aws/t3.large/montage__0.25__1.0.0__2020-06-02-14-53-10\n",
      "Loading from ../data/parsed-logs/aws/t3.large/montage2__1.0__1.0.0__2020-06-21-09-35-33\n",
      "Loading from ../data/parsed-logs/aws/t3.large/soykb__238__1.0.0__2020-06-02-20-48-07\n",
      "Loading from ../data/parsed-logs/aws/t3.large/montage__0.25__1.0.0__2020-06-01-18-33-27\n",
      "Loading from ../data/parsed-logs/aws/t3.large/montage__1.0__1.0.0__2020-06-01-17-38-48\n",
      "Loading from ../data/parsed-logs/aws/t3.large/montage2__0.25__1.0.0__2020-06-01-18-26-34\n",
      "Loading from ../data/parsed-logs/aws/t3.large/soykb__446__1.0.0__2020-06-03-08-22-30\n",
      "Loading from ../data/parsed-logs/aws/t3.large/montage__0.25__1.0.0__2020-06-01-17-30-49\n",
      "Loading from ../data/parsed-logs/aws/t3.large/soykb__82__1.0.0__2020-06-20-22-34-05\n",
      "Loading from ../data/parsed-logs/aws/t3.large/soykb__238__1.0.0__2020-06-03-14-14-17\n",
      "Loading from ../data/parsed-logs/aws/t3.large/montage2__1.0__1.0.0__2020-06-02-07-29-50\n",
      "Loading from ../data/parsed-logs/aws/t3.large/soykb__446__1.0.0__2020-06-04-02-04-46\n",
      "Loading from ../data/parsed-logs/aws/t3.large/montage2__0.01__1.0.0__2020-06-01-19-15-30\n",
      "Loading from ../data/parsed-logs/aws/t3.large/soykb__134__1.0.0__2020-06-21-01-30-50\n",
      "Loading from ../data/parsed-logs/aws/t3.large/soykb__56__1.0.0__2020-06-20-20-51-46\n",
      "Loading from ../data/parsed-logs/aws/t3.large/montage__1.0__1.0.0__2020-06-01-18-41-41\n",
      "Loading from ../data/parsed-logs/aws/t3.large/montage2__0.01__1.0.0__2020-06-01-18-14-25\n",
      "Loading from ../data/parsed-logs/aws/t3.2xlarge/montage2__1.0__1.0.0__2020-06-20-21-47-34\n",
      "Loading from ../data/parsed-logs/aws/t3.2xlarge/montage__1.0__1.0.0__2020-06-20-20-00-50\n",
      "Loading from ../data/parsed-logs/aws/t3.2xlarge/soykb__82__1.0.0__2020-05-28-00-15-38\n",
      "Loading from ../data/parsed-logs/aws/t3.2xlarge/soykb__238__1.0.0__2020-06-21-03-17-40\n",
      "Loading from ../data/parsed-logs/aws/t3.2xlarge/soykb__82__1.0.0__2020-05-27-22-51-27\n",
      "Loading from ../data/parsed-logs/aws/t3.2xlarge/soykb__134__1.0.0__2020-05-28-12-49-33\n",
      "Loading from ../data/parsed-logs/aws/t3.2xlarge/soykb__446__1.0.0__2020-06-22-07-02-48\n",
      "Loading from ../data/parsed-logs/aws/t3.2xlarge/montage__0.25__1.0.0__2020-06-20-18-53-52\n",
      "Loading from ../data/parsed-logs/aws/t3.2xlarge/montage2__0.01__1.0.0__2020-06-20-20-36-10\n",
      "Loading from ../data/parsed-logs/aws/t3.2xlarge/soykb__82__1.0.0__2020-05-28-03-04-08\n",
      "Loading from ../data/parsed-logs/aws/t3.2xlarge/soykb__238__1.0.0__2020-06-21-21-35-26\n",
      "Loading from ../data/parsed-logs/aws/t3.2xlarge/soykb__446__1.0.0__2020-06-21-12-49-17\n",
      "Loading from ../data/parsed-logs/aws/t3.2xlarge/montage__2.0__1.0.0__2020-06-20-19-23-36\n",
      "Loading from ../data/parsed-logs/aws/t3.2xlarge/soykb__82__1.0.0__2020-05-27-21-27-16\n",
      "Loading from ../data/parsed-logs/aws/t3.2xlarge/soykb__56__1.0.0__2020-05-27-18-52-48\n",
      "Loading from ../data/parsed-logs/aws/t3.2xlarge/montage__0.25__1.0.0__2020-06-20-19-53-09\n",
      "Loading from ../data/parsed-logs/aws/t3.2xlarge/soykb__134__1.0.0__2020-05-28-07-58-11\n",
      "Loading from ../data/parsed-logs/aws/t3.2xlarge/soykb__134__1.0.0__2020-05-28-15-18-05\n",
      "Loading from ../data/parsed-logs/aws/t3.2xlarge/soykb__82__1.0.0__2020-05-28-01-40-55\n",
      "Loading from ../data/parsed-logs/aws/t3.2xlarge/montage2__0.01__1.0.0__2020-06-20-19-36-56\n",
      "Loading from ../data/parsed-logs/aws/t3.2xlarge/soykb__56__1.0.0__2020-05-27-19-49-25\n",
      "Loading from ../data/parsed-logs/aws/t3.2xlarge/montage2__0.25__1.0.0__2020-06-20-20-45-52\n",
      "Loading from ../data/parsed-logs/aws/t3.2xlarge/montage__1.0__1.0.0__2020-06-20-19-01-30\n",
      "Loading from ../data/parsed-logs/aws/t3.2xlarge/montage2__0.25__1.0.0__2020-06-20-19-45-49\n",
      "Loading from ../data/parsed-logs/aws/t3.2xlarge/soykb__134__1.0.0__2020-05-28-10-24-03\n",
      "Loading from ../data/parsed-logs/aws/t3.2xlarge/montage2__1.0__1.0.0__2020-05-27-11-54-14\n",
      "Loading from ../data/parsed-logs/aws/t3.2xlarge/montage__2.0__1.0.0__2020-06-20-20-23-08\n",
      "Loading from ../data/parsed-logs/aws/t3.2xlarge/soykb__134__1.0.0__2020-05-28-05-30-50\n"
     ]
    }
   ],
   "source": [
    "df = load_frames_for_base(data_root, \"aws\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta, events = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_events = events.loc[events[\"parameter\"] != \"event\"].reset_index()\n",
    "# filtered_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_events.parameter.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\"read\": 1225, \"write\": 1, \"readSyscalls\": 5, \"writeSyscalls\": 1, \"readReal\": 0, \"writeReal\": 0, \"writeCancelled\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "io_params = [\"read\", \"write\", \"readSyscalls\", \"writeSyscalls\", \"readReal\", \"writeReal\", \"writeCancelled\"]\n",
    "network_params = [\"rxBytes\", \"rxPackets\", \"rxErrors\", \"rxDrop\", \"rxFifo\", \"rxFrame\", \"rxCompressed\", \"rxMulticast\", \"txBytes\", \"txPackets\", \"txErrors\", \"txDrop\", \"txFifo\", \"txColls\", \"txCarrier\", \"txCompressed\"]\n",
    "\n",
    "io_agg_kwargs={f\"{param}_sum\" : (f\"value.{param}\", \"sum\") for param in io_params}\n",
    "network_agg_kwargs={f\"{param}_sum\" : (f\"value.{param}\", \"sum\") for param in network_params}\n",
    "cpu_agg_kwargs={\"cpu_mean\": ('value', 'mean'), \"cpu_max\": ('value', 'max')}\n",
    "memory_agg_kwargs={\"memory_mean\": ('value', 'mean'), \"memory_max\": ('value', 'max')}\n",
    "ctime_agg_kwargs={\"ctime_mean\": ('value', 'mean'), \"ctime_max\": ('value', 'max'), \"ctime_sum\": ('value', 'sum')}\n",
    "\n",
    "def extract_meta_from_metrics(frame):\n",
    "    frame[\"value\"] = pd.to_numeric(frame[\"value\"])\n",
    "    def postprocess(aggregated_frame, parameter_type):\n",
    "        aggregated_frame = aggregated_frame.reset_index()\n",
    "        return aggregated_frame.loc[aggregated_frame[\"parameter\"] == parameter_type].drop(\"parameter\", axis=1)\n",
    "    \n",
    "    frame_grp = frame.groupby(['jobId', 'parameter'])\n",
    "    metrics = [\n",
    "        postprocess(frame_grp.agg(**io_agg_kwargs), \"io\"),\n",
    "        postprocess(frame_grp.agg(**network_agg_kwargs), \"network\"),          \n",
    "        postprocess(frame_grp.agg(**cpu_agg_kwargs), \"cpu\"),\n",
    "        postprocess(frame_grp.agg(**memory_agg_kwargs), \"memory\"),\n",
    "        postprocess(frame_grp.agg(**ctime_agg_kwargs), \"ctime\")   \n",
    "    ]   \n",
    "    return reduce(lambda left, right: pd.merge(left,right,on='jobId'), metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_meta_from_metrics(filtered_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.rcParams['figure.figsize'] = [10, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta.groupby('executable').count().sort_values('size').plot.barh(y='size', logx=True, legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo - zmerge'ować base z metadanymi. Zrobić matrycę pearsona. Podstawowe klasyfikatory. GPT-2. Podział na "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "workflowName                                                       montage\n",
       "size                                                                   1.0\n",
       "version                                                              1.0.0\n",
       "hyperflowId                                                      0EPinjEE9\n",
       "jobId                                                      0EPinjEE9-1-225\n",
       "executable                                                        mDiffFit\n",
       "args                     [-s, fit.000044.000076.txt, p2mass-atlas-00020...\n",
       "inputs                   [{'name': 'big_region_20180402_165223_16974.hd...\n",
       "outputs                  [{'name': 'fit.000044.000076.txt'}, {'name': '...\n",
       "name                                                              mDiffFit\n",
       "command                  mDiffFit -s fit.000044.000076.txt p2mass-atlas...\n",
       "execTimeMs                                                              59\n",
       "cpu.manufacturer                                                    Intel®\n",
       "cpu.brand                                             Xeon® Platinum 8124M\n",
       "cpu.vendor                                                    GenuineIntel\n",
       "cpu.family                                                               6\n",
       "cpu.model                                                               85\n",
       "cpu.stepping                                                             4\n",
       "cpu.revision                                                              \n",
       "cpu.voltage                                                               \n",
       "cpu.speed                                                             3.00\n",
       "cpu.speedmin                                                              \n",
       "cpu.speedmax                                                              \n",
       "cpu.governor                                                              \n",
       "cpu.cores                                                                4\n",
       "cpu.physicalCores                                                        2\n",
       "cpu.processors                                                           1\n",
       "cpu.socket                                                                \n",
       "cpu.cache.l1d                                                        32768\n",
       "cpu.cache.l1i                                                        32768\n",
       "cpu.cache.l2                                                       1048576\n",
       "cpu.cache.l3                                                      25952256\n",
       "mem.total                                                       7994712064\n",
       "mem.free                                                        2274062336\n",
       "mem.used                                                        5720649728\n",
       "mem.active                                                       935063552\n",
       "mem.available                                                   7059648512\n",
       "mem.buffers                                                        2768896\n",
       "mem.cached                                                      4805009408\n",
       "mem.slab                                                         468758528\n",
       "mem.buffcache                                                   5276536832\n",
       "mem.swaptotal                                                            0\n",
       "mem.swapused                                                             0\n",
       "mem.swapfree                                                             0\n",
       "stdout                                                                 NaN\n",
       "nodeName                                                               NaN\n",
       "env.podIp                                                              NaN\n",
       "env.nodeName                                                           NaN\n",
       "env.podName                                                            NaN\n",
       "env.podServiceAccount                                                  NaN\n",
       "env.podNamespace                                                       NaN\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta.iloc[0]\n",
    "# meta.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_metrics(events_frame):\n",
    "    print(\"Preprocessing metrics\")\n",
    "    events_frame = events_frame.loc[events_frame[\"parameter\"] != \"event\"].reset_index()\n",
    "    return extract_meta_from_metrics(events_frame)\n",
    "\n",
    "def preprocess_meta(meta_frame):\n",
    "    print(\"Preprocessing meta\")\n",
    "    return meta_frame.drop([\"hyperflowId\", 'version', 'nodeName', 'cpu.socket', 'cpu.speedmin', \n",
    "                            'cpu.speedmax', 'cpu.governor', 'cpu.revision', 'cpu.voltage', 'env.nodeName', \n",
    "                            'env.podIp', 'env.podServiceAccount', 'env.podName', 'env.podNamespace', 'stdout'], axis=1)\n",
    "\n",
    "def join_metrics_with_meta(metrics_frame, meta_frame):\n",
    "    print(\"Joining metrics with meta\")\n",
    "    return pd.merge(metrics_frame, meta_frame, on=\"jobId\")\n",
    "\n",
    "def dump_preprocessed(data_frame, path=\"merged.csv\"):\n",
    "    print(\"Dumping preprocessed\")\n",
    "    data_frame.to_csv(os.path.join(data_root, path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_intermediate():\n",
    "    metrics = preprocess_metrics(events)\n",
    "    metadata = preprocess_meta(meta)\n",
    "    joint_df = join_metrics_with_meta(metrics, metadata)\n",
    "    dump_preprocessed(joint_df, \"aws.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing metrics\n",
      "Preprocessing meta\n",
      "Joining metrics with meta\n",
      "Dumping preprocessed\n"
     ]
    }
   ],
   "source": [
    "store_intermediate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
